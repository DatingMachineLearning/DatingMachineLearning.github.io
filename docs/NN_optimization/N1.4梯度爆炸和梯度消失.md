# 1.4 梯度爆炸和梯度消失

## 问题出现

在梯度函数上出现的以指数级递增或者递减的情况分别称为**梯度爆炸**或者**梯度消失**。

假定 $g(z) = z, b^{[l]} = 0$，对于目标输出有：

$$
\hat{y} = W^{[L]}W^{[L-1]}...W^{[2]}W^{[1]}X
$$

* 对于 $W^{[l]}$的值大于 1 的情况，激活函数的值将以指数级递增；
* 对于 $W^{[l]}$的值小于 1 的情况，激活函数的值将以指数级递减。

对于导数同理。因此，在计算梯度时，根据不同情况梯度函数会以指数级递增或递减，导致训练导数难度上升，梯度下降算法的步长会变得非常小，需要训练的时间将会非常长。

微软在152层的网络上有实验，这样的过深的网络导致最后的代价函数梯度很大或者很小。

### 梯度消失

在神经网络反向传播中，当梯度从后往前传时，梯度不断减小，最后变为零，此时，浅层的神经网络权重得不到更新，那么前面隐藏层的学习速率低于后面隐藏层的学习速率，即随着隐藏层数目的增加，分类准确率反而下降了。这种现象叫做梯度消失。

梯度消失导致后层的权重更新的快，靠近输出层的权值更新相对正常，而前层网络由于梯度传递不过去而得不到更新。靠近输入层的权值更新会变得很慢，导致靠近输入层的隐藏层权值几乎不变，接近于初始化的权值。这样在网络很深的时候，学习的速度很慢或者无法学习。

梯度消失导致神经网络的hidden layer 1 相当于只是一个映射层，对所有的输入做了一个函数映射，这时此深度神经网络的学习就等价于只有后几层的隐藏层网络在学习，就发挥不到深层神经网络的效果了。

### 梯度爆炸

当权值过大，神经网络前面层比后面层梯度变化更快，会引起**梯度爆炸**问题。梯度爆炸就是由于初始化权值过大，w大到乘以激活函数的导数都大于1，因为前面层比后面层变化的更快，就会导致神经网络前面层的权值越来越大，梯度爆炸的现象就发生了。

梯度爆炸是一种与梯度消失相反的情况，当进行反向传播时，梯度从后往前传时，梯度不断增大，导致权重更新太大，以致于不断波动，使神经网络在最优点之间波动。

### 解决方法

1. 用 ReLU、Leaky-ReLU、P-ReLU、R-ReLU、Maxout 等替代sigmoid函数。
2. 用 Batch Normalization。
3. LSTM的结构设计可以改善RNN中的梯度消失问题。
4. 动态地改变学习率，当梯度过小时，增大学习率，当过大时，减小学习率。
5. 神经网络的权重标准初始化

## 用初始化缓解梯度消失和爆炸

根据

$$
z={w}_1{x}_1+{w}_2{x}_2 + ... + {w}_n{x}_n + b
$$
可知，当输入的数量 n 较大时，我们希望每个 wi 的值都小一些，这样它们的和得到的 z 也较小。

为了得到较小的 $w_i$，设置 $\mathrm {Var}(w_i)= \frac{1}{n}$，这里称为 **Xavier initialization**。

```python
WL = np.random.randn(WL.shape[0], WL.shape[1]) * np.sqrt(1 / n)
```

其中 n 是输入的神经元个数，即`WL.shape[1]`。

这样，激活函数的输入 x 近似设置成均值为 0，标准方差为 1，神经元输出 z 的方差就正则化到 1 了。虽然没有解决梯度消失和爆炸的问题，但其在一定程度上确实减缓了梯度消失和爆炸的速度。

在 ReLU activation function 中推荐使用 Xavier Initialization的变种，暂且称之为 He Initialization： 它和  Xavier initialization 唯一的区别是 $\mathrm {Var}(w_i)= \frac{2}{n}$ 

当激活函数使用 ReLU 时，使用 $\mathrm {Var}(w_i)= \frac{2}{n}$ ；

当激活函数使用 tanh 时，使用 $\mathrm {Var}(w_i)= \frac{1}{n}$ 。

Weight Initialization matters ！深度学习中的 weight initialization 对模型收敛速度和模型质量有重要影响！

### 可行的几种初始化方式

> https://zhuanlan.zhihu.com/p/25110150

#### pre-training

pre-training是早期训练神经网络的有效初始化方法，一个便于理解的例子是先使用greedy layerwise auto-encoder做unsupervised pre-training，然后再做fine-tuning。具体过程可以参见UFLDL的一个[tutorial](https://link.zhihu.com/?target=http%3A//ufldl.stanford.edu/wiki/index.php/Stacked_Autoencoders)，因为这不是本文重点，就在这里简略的说一下：（1）pre-training阶段，将神经网络中的每一层取出，构造一个auto-encoder做训练，使得输入层和输出层保持一致。在这一过程中，参数得以更新，形成初始值（2）fine-tuning阶段，将pre-train过的每一层放回神经网络，利用pre-train阶段得到的参数初始值和训练数据对模型进行整体调整。在这一过程中，参数进一步被更新，形成最终模型。

随着数据量的增加以及activation function (参见我的另一篇[文章](https://zhuanlan.zhihu.com/p/25110450)) 的发展，pre-training的概念已经渐渐发生变化。目前，从零开始训练神经网络时我们也很少采用auto-encoder进行pre-training，而是直奔主题做模型训练。不想从零开始训练神经网络时，我们往往选择一个已经训练好的在任务A上的模型（称为pre-trained model），将其放在任务B上做模型调整（称为fine-tuning）。

#### random initialization

随机初始化是很多人目前经常使用的方法，然而这是有弊端的，一旦随机分布选择不当，就会导致网络优化陷入困境。

#### Xavier initialization

Xavier initialization可以解决上面的问题！其初始化方式也并不复杂。Xavier初始化的基本思想是保持输入和输出的方差一致，这样就避免了所有输出值都趋向于0。注意，为了问题的简便，Xavier初始化的推导过程是基于线性函数的，但是它在一些非线性神经元中也很有效。

#### He initialization

He initialization的思想是：在 ReLU 网络中，假定每一层有一半的神经元被激活，另一半为0，所以，要保持variance不变，只需要在Xavier的基础上再除以2。

#### Batch Normalization Layer

Batch Normalization是一种巧妙而粗暴的方法来削弱bad initialization的影响，其基本思想是：If you want it, just make it!

我们想要的是在非线性activation之前，输出值应该有比较好的分布（例如高斯分布），以便于back propagation时计算gradient，更新weight。Batch Normalization将输出值强行做一次Gaussian Normalization和线性变换：

…… 

> https://zhuanlan.zhihu.com/p/25110150

## 梯度检验 (Gradient checking)

### 原理

使用双边误差的方法去逼近导数，精度要高于单边误差。我们可以利用双边误差去检验梯度是否计算正确。

单边误差：

![one-sided-difference](https://gitee.com/xrandx/blog-figurebed/raw/master/img/20210627111637.png)
$$
f'(\theta) = {\lim_{\varepsilon\to 0}} = \frac{f(\theta + \varepsilon) - (\theta)}{\varepsilon}
$$
误差：$O(\varepsilon)$

双边误差求导（即导数的定义）：

![two-sided-difference](https://gitee.com/xrandx/blog-figurebed/raw/master/img/20210627111538.png)

$$
f'(\theta) = {\lim_{\varepsilon\to 0}} = \frac{f(\theta + \varepsilon) - (\theta - \varepsilon)}{2\varepsilon}
$$
误差：$O(\varepsilon^2)$

当 ε 越小时，结果越接近真实的导数，也就是梯度值。可以使用这种方法来判断反向传播进行梯度下降时，是否出现了错误。

### 实现

将 $W^{[1]}$，$b^{[1]}$，...，$W^{[L]}$，$b^{[L]}$全部连接起来，成为一个巨型向量 θ。代入损失函数，$J(W^{[1]}, b^{[1]}, ..., W^{[L]}，b^{[L]}) = J(\theta)$

同时，对 $\mathrm d W^{[1]}$，$\mathrm db^{[1]}$，...，$\mathrm dW^{[L]}$，$\mathrm db^{[L]}$执行同样的操作得到巨型向量 $\mathrm d \theta$，它和 θ 有同样的维度。

求得一个梯度逼近值：

$$
\mathrm d\theta_{approx}[i] ＝ \frac{J(\theta_1, \theta_2, ..., \theta_i+\varepsilon, ...) - J(\theta_1, \theta_2, ..., \theta_i-\varepsilon, ...)}{2\varepsilon}
$$
它应该近似于 $\mathrm d \theta[i]$ 。

为了提高计算速度，以及防止运算 overflow，下列梯度检验值应该近似于 $\theta$ ，判断是否接近 $\theta$ 就等于判断梯度是否正确：
$$
\frac{{||d\theta_{approx} - d\theta||}_2}{{||d\theta_{approx}||}_2+{||d\theta||}_2} \approx \theta
$$
如果上式成立，说明神经网络梯度正确，否则要去检查代码是否存在 bug。

### 梯度检验注意事项

1. 不要在训练中使用梯度检验，它只用于调试（debug）。使用完毕应该关闭梯度检验的功能，
2. 如果算法的梯度检验失败，要检查所有项，并试着找出 bug，即确定哪个 $\mathrm dθ_{\text approx}[i]$ 与 $\mathrm d θ$ 的值相差比较大；
3. 当成本函数包含正则项时，需要带上正则项进行检验；
4. 梯度检验不能与 dropout 同时使用。因为每次迭代过程中，dropout 会随机消除隐藏层单元的不同子集，难以计算 dropout 在梯度下降上的成本函数 J。建议关闭 dropout，用梯度检验进行双重检查，确定在没有 dropout 的情况下算法正确，然后打开 dropout；

