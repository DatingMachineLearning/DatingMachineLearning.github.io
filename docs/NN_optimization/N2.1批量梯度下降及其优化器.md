# 2.1 梯度下降及其优化器

深度学习难以在大数据领域发挥最大效果的一个原因是，在巨大的数据集基础上进行训练速度很慢。而优化算法能够帮助快速训练模型，大大提高效率。

## 批量梯度下降法 Batch Gradient Descent (BGD)

**batch 梯度下降法**（批梯度下降法，我们之前一直使用的梯度下降法）是最常用的梯度下降形式，即同时处理整个训练集。其在更新参数时使用所有的样本来进行更新。

对整个训练集进行梯度下降法的时候，我们必须处理整个训练数据集，然后才能进行一步梯度下降，即每一步梯度下降法需要对整个训练集进行一次处理，如果训练数据集很大的时候，处理速度就会比较慢。

但是如果每次处理训练数据的一部分即进行梯度下降法，则我们的算法速度会执行的更快。而处理的这些一小部分训练子集即称为 **mini-batch**。

## 随机梯度下降 Stochastic Gradient Descent (SGD)

随机梯度下降是通过每个样本来迭代更新一次，如果样本量很大的情况，那么可能只用其中部分的样本，就已经将theta迭代到最优解了，对比上面的批量梯度下降，迭代一次需要用到十几万训练样本，一次迭代不可能最优，如果迭代10次的话就需要遍历训练样本10次。缺点是SGD的噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向。所以虽然训练速度快，但是准确度下降，并不是全局最优。虽然包含一定的随机性，但是从期望上来看，它是等于正确的导数的。

缺点：

SGD 因为更新比较频繁，会造成 cost function 有严重的震荡。

BGD 可以收敛到局部极小值，当然 SGD 的震荡可能会跳到更好的局部极小值处。

当我们稍微减小 learning rate，SGD 和 BGD 的收敛性是一样的。

> [深度学习——优化器算法Optimizer详解（BGD、SGD、MBGD、Momentum、NAG、Adagrad、Adadelta、RMSprop、Adam） - 郭耀华 - 博客园](https://www.cnblogs.com/guoyaohua/p/8542554.html)

## 小批量梯度下降法 Mini-Batch Gradient Descent (MBGD)

小批量梯度下降法每次同时处理**单个**的 mini-batch，其他与 batch 梯度下降法一致。

使用 batch 梯度下降法，对整个训练集的一次遍历只能做一个梯度下降；而使用 Mini-Batch 梯度下降法，对整个训练集的一次遍历（称为一个 epoch）能做 mini-batch 的个数的梯度下降。之后，可以一直遍历训练集，直到最后收敛到一个合适的精度。

batch 梯度下降法和 Mini-batch 梯度下降法代价函数的变化趋势如下：

![training-with-mini-batch-gradient-descent](https://gitee.com/xrandx/blog-figurebed/raw/master/img/20210627153553.png)

### batch 的不同大小带来的影响

* mini-batch 的大小为 1，即是**随机梯度下降法（stochastic gradient descent）**，每个样本都是独立的 mini-batch。看到多了随机两个字，随机也就是说我们用样本中的一个例子来近似所有的样本；
* mini-batch 的大小为 m，即是 batch 梯度下降法；

![choosing-mini-batch-size](https://gitee.com/xrandx/blog-figurebed/raw/master/img/20210627153619.png)

* batch 梯度下降法：
  * 对所有 m 个训练样本执行一次梯度下降，**每一次迭代时间较长，训练过程慢**； 
  * 相对噪声低一些，幅度也大一些；
  * 成本函数总是向减小的方向下降。

* 随机梯度下降法：
  * 对每一个训练样本执行一次梯度下降，训练速度快，但**丢失了向量化带来的计算加速**；
  * 有很多噪声，减小学习率可以适当；
  * 成本函数总体趋势向全局最小值靠近，但永远不会收敛，而是一直在最小值附近波动。

因此，选择一个`1 < size < m`的合适的大小进行 Mini-batch 梯度下降，可以实现快速学习，也应用了向量化带来的好处，且成本函数的下降处于前两者之间。

### mini-batch 大小的选择

* 如果训练样本的大小比较小，如 $m ⩽ 2000$ 时，选择 batch 梯度下降法；
* 如果训练样本的大小比较大，选择 Mini-Batch 梯度下降法。为了和计算机的信息存储方式相适应，代码在 mini-batch 大小为 2 的幂次时运行要快一些。典型的大小为 $2^6$、$2^7$、...、$2^9$；
* mini-batch 的大小要符合 CPU/GPU 内存。

mini-batch 的大小也是一个重要的超参数，需要根据经验快速尝试，找到能够最有效地减少成本函数的值。

### 问题

1. 随机梯度下降的 batch-size设置成多少合适？过小有什么问题？过大有什么问题？提示：过大以整个样本集合为例，过小以单个样本为例来思考。

2. 一次训练使用的配置：5个epoch，1000个样本，batch-size = 20，最内层循环执行多少轮？

### 答案

> 1. 128（batch size 需要调参）
>
>    过大：
>
>    - batch size太大，memory容易不够用。这个很显然，就不多说了。
>
>    - batch size太大，深度学习的优化（training loss降不下去）和泛化（generalization gap很大）都会出问题。
>
>    过小：
>
>    - 当你的 batch size太小的时候，在一定的epoch数里，训练出来的参数 posterior 是根本来不及接近 long-time limit 的定态分布。来不及收敛。
>
>    合适的batch size范围主要和收敛速度、随机梯度噪音有关。[怎么选取训练神经网络时的Batch size? - 知乎 (zhihu.com)](https://www.zhihu.com/question/61607442)
>
>    [训练神经网络时如何确定batch的大小？ (qq.com)](https://mp.weixin.qq.com/s?__biz=MzIwNzc2NTk0NQ==&mid=2247484570&idx=1&sn=4c0b6b76a7f2518d77818535b677e087&chksm=970c2c4ca07ba55ad5cfe6b46f72dbef85a159574fb60b9932404e45747c95eed8c6c0f66d62#rd)
>
> 2. 最外层 5 次，最内层 50 次。

## 参考

吴恩达，深度学习课程
