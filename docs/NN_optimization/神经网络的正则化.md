# 正则化方法

。。。

待补充



## 常用的正则化

### 对数几率回归正则化

$$
J(\mathbf{w},b) = \frac{1}{m}\sum_{i=1}^mL(\hat{y}^{(i)},y^{(i)})+\frac{\lambda}{2m}{||\mathbf{w}||}^2_2
$$

L2 正则化
$$
\frac{\lambda}{2m}{||\mathbf w||}^2_2 = \frac{\lambda}{2m}\sum_{j=1}^{n}w^2_j = \frac{\lambda}{2m}w^Tw
$$
L1 正则化
$$
\frac{\lambda}{2m}{||\mathbf w||}_1 = \frac{\lambda}{2m}\sum_{j=1}^{n}{|w_j|}
$$
其中，λ 为正则化因子，用于权衡正则化的程度。

由于 L1 正则化最后得到 w 向量中将存在大量的 0，使模型变得稀疏化，因此 L2 正则化更加常用。

### 神经网络正则化

### 范数

$$
J(\mathbf{W}^{[1]}, \mathbf{b}^{[1]}, ..., \mathbf{W}^{[L]}, \mathbf{b}^{[L]}) = \frac{1}{m}\sum_{i=1}^mL(\mathbf{\hat{y}}^{(i)},\mathbf{y}^{(i)})+\frac{\lambda}{2m}\sum_{l=1}^L{{||\mathbf{W}^{[l]}||}}^2_F
$$

$\bf W$ 的大小是 $(n^{[l - 1]}, n^{[l]})$ 所以：
$$
{{||\mathbf W^{[l]}||}}^2_F = \sum^{n^{[l-1]}}_{i=1}\sum^{n^{[l]}}_{j=1}(\mathbf W^{[l]}_{ij})^2
$$
该矩阵范数被称为弗罗贝尼乌斯范数(Frobenius Norm)，所以神经网络中的正则化项被称为弗罗贝尼乌斯范数矩阵。

加入范数之后，梯度就变成了：
$$
dW^{[l]}= \frac{\partial L}{\partial \mathbf W ^{[l]}} +\frac{\lambda}{m}\mathbf W^{[l]}
$$
根据梯度下降法我们知道：
$$
\begin{aligned}
W^{[l]} &:= W^{[l]} - \alpha \frac{\partial L}{\partial W^{[l]}} \\
&= W^{[l]} - \alpha \frac{\lambda}{m}W^{[l]} - \alpha \mathrm dW^{[l]} \\
&= (1 - \frac{\alpha\lambda}{m})W^{[l]} - \alpha \mathrm dW^{[l]}
\end{aligned}
$$
由于 $1 - \frac{\alpha\lambda}{m}<1$ ，所以权值实际是缩小的，因此 L2 正则化项也被称为**权重衰减（Weight Decay）**。

#### 正则化降低复杂度的原因

#### 直观解释

正则化因子设置的足够大的情况下，为了使成本函数最小化，权重矩阵 W 就会被设置为接近于 0 的值，直观上相当于消除了很多神经元的影响，那么大的神经网络就会变成一个较小的网络。当然，实际上隐藏层的神经元依然存在，但是其影响减弱了，便不会导致过拟合。

假设神经元中使用的激活函数为 $g(z) = \mathrm {tanh}(z)$（sigmoid 同理）。

![regularization_prevent_overfitting](https://gitee.com/xrandx/blog-figurebed/raw/master/img/20210624151428.png)

在加入正则化项后，当 λ 增大，导致 $W^{[l]}$减小，$Z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]}$便会减小。由上图可知，在 z 较小（接近于 0）的区域里，`tanh(z)`函数近似线性，所以每层的函数就近似线性函数，整个网络就成为一个简单的近似线性的网络，因此不会发生过拟合。

在权值 $w^{[L]}$变小之下，输入样本 X 随机的变化不会对神经网络模造成过大的影响，神经网络受局部噪音的影响的可能性变小。这就是正则化能够降低模型方差的原因。

### Dropout

Dropout (随机失活)是在神经网络的隐藏层为每个神经元结点设置一个随机消除的概率，保留下来的神经元形成一个结点较少、规模较小的网络用于训练。

#### Inverted Dropout

反向随机失活是实现 dropout 的方法。对第`l`层进行 dropout：

```python
keep_prob = 0.8    # 设置神经元保留概率
dl = np.random.rand(al.shape[0], al.shape[1]) < keep_prob
al = np.multiply(al, dl)
al /= keep_prob
```

最后一步`al /= keep_prob`是因为 $a^{[l]}$ 中的一部分元素失活（相当于被归零），为了在下一层计算时不影响 $Z^{[l+1]} = W^{[l+1]}a^{[l]} + b^{[l+1]}$ 的期望值，因此除以一个`keep_prob`。

在测试阶段不要使用 dropout，因为那样会使得预测结果变得随机。

#### 直觉理解

对于单个神经元，其工作是接收输入并产生一些有意义的输出。但是加入了 dropout 后，输入的特征都存在被随机清除的可能，所以该神经元不会再特别依赖于任何一个输入特征，即不会给任何一个输入特征设置太大的权重。

因此，通过传播过程，dropout 将产生和 L2 正则化相同的**收缩权重**的效果。

对于不同的层，设置的`keep_prob`也不同。一般来说，神经元较少的层，会设`keep_prob`为 1.0，而神经元多的层则会设置比较小的`keep_prob`。

dropout 的一大**缺点**是成本函数无法被明确定义。因为每次迭代都会随机消除一些神经元结点的影响，因此无法确保成本函数单调递减。因此，使用 dropout 时，先将`keep_prob`全部设置为 1.0 后运行代码，确保 $J(w, b)$函数单调递减，再打开 dropout。

## 其他正则化方法

#### 数据增强 (Data Augmentation)

* 通过图片的一些变换（翻转，局部放大后切割等），得到更多的训练集和验证集。

#### 早停法 (Early Stopping)

将训练集和验证集进行梯度下降时的成本变化曲线画在同一个坐标轴内，当训练集误差降低但验证集误差升高，两者开始发生较大偏差时及时停止迭代，并返回具有最小验证集误差的连接权和阈值，以避免过拟合。这种方法的缺点是无法同时达成偏差和方差的最优。



