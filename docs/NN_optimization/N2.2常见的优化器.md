# 2.2 常见的优化器

## RMSProp 算法

RMSProp（Root Mean Square Propagation，均方根传播）算法是在对梯度进行指数加权平均的基础上，引入平方和平方根。具体过程为：
$$
\begin{aligned}
s_{\mathrm dW} &= \beta s_{\mathrm dW} + (1 - \beta)(\mathrm d W)^2 \\
s_{\mathrm db} &= \beta s_{\mathrm db} + (1 - \beta)(\mathrm d b)^2 \\ 
W &:= W - \alpha \frac{\mathrm dW}{\sqrt{s_{\mathrm dW} + \epsilon}} \\
b &:= b - \alpha \frac{\mathrm db}{\sqrt{s_{\mathrm db} + \epsilon}} \\
\end{aligned}
$$


其中，ϵ 是一个实际操作时加上的较小数（例如$10^{-8}$），为了防止分母太小而导致的数值不稳定。

当 dW 或 db 较大时，$(\mathrm dW)^2$、$(\mathrm db)^2$会较大，进而 $s_{\mathrm d w}$、$s_{\mathrm d b}$也会较大，最终使得 $\frac{\mathrm d w}{\sqrt{s_{\mathrm d w} + \epsilon}}$ 和 $\frac{\mathrm db}{\sqrt{s_{\mathrm d b} + \epsilon}}$ 较小，从而减小某些维度梯度更新波动较大的情况，使下降速度变得更快。

![RMSProp](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/RMSProp.png)

RMSProp 有助于减少抵达最小值路径上的摆动，并允许使用一个更大的学习率 α，从而加快算法学习速度。并且，它和 Adam 优化算法已被证明适用于不同的深度学习网络结构。

注意，β 也是一个超参数。

## Adam 优化算法

Adam 优化算法（Adaptive Moment Estimation，自适应矩估计）基本上就是将 Momentum 和 RMSProp 算法结合在一起，结合了 AdaGrad 和 RMSProp 的优点。Adam 对每个参数使用相同的学习率，并随着学习的进行而独立地适应。此外，Adam 是基于动量的算法，利用了梯度的历史信息。基于这些特征，在选择优化算法时，Adam 往往是「当仁不让」。具体过程如下：

首先进行初始化，

$$
v_{\mathrm dW} = 0, s_{\mathrm dW} = 0, v_{\mathrm d b} = 0, s_{\mathrm d b} = 0
$$
用每一个 mini-batch 计算 $\mathrm dW$、$\mathrm d b$，第 t 次迭代时：
$$
\begin{aligned}
v_{\mathrm dW} &= \beta_1 v_{\mathrm dW} + (1 - \beta_1) \mathrm dW \\ 
v_{\mathrm d b} &= \beta_1 v_{\mathrm d b} + (1 - \beta_1) \mathrm d b \\ 
s_{\mathrm dW} &= \beta_2 s_{\mathrm dW} + (1 - \beta_2) {(\mathrm dW)}^2 \\ 
s_{\mathrm d b} &= \beta_2 s_{\mathrm d b} + (1 - \beta_2) {(\mathrm d b)}^2 \\
\end{aligned}
$$
一般使用 Adam 算法时需要计算偏差修正：
$$
\begin{aligned}
v^{\text{corrected}}_{\mathrm dW} &= \frac{v_{\mathrm dW}}{1-{\beta_1}^t} \\
v^{\text{corrected}}_{\mathrm d b} &= \frac{v_{\mathrm d b}}{1-{\beta_1}^t} \\
s^{\text{corrected}}_{\mathrm dW} &= \frac{s_{\mathrm dW}}{1-{\beta_2}^t} \\
s^{\text{corrected}}_{\mathrm d b} &= \frac{s_{\mathrm d b}}{1-{\beta_2}^t} \\
\end{aligned}
$$
所以，更新 W、b 时有：

$$
W := W - \alpha \frac{v^{\text{corrected}}_{\mathrm dW}}{{\sqrt{s^{\text{corrected}}_{\mathrm dW}} + \epsilon}}
\\
b := b - \alpha \frac{v^{\text{corrected}}_{\mathrm d b}}{{\sqrt{s^{\text{corrected}}_{\mathrm d b}} + \epsilon}}
$$
（可以看到 Andrew 在这里 ϵ 没有写到平方根里去，和他在 RMSProp 中写的不太一样。考虑到 ϵ 所起的作用，我感觉影响不大）

### 超参数的选择

Adam 优化算法有很多的超参数，其中

* 学习率 α：需要尝试一系列的值，来寻找比较合适的；
* β1：常用的缺省值为 0.9；
* β2：Adam 算法的作者建议为 0.999；
* ϵ：不重要，不会影响算法表现，Adam 算法的作者建议为 $10^{-8}$；

β1、β2、ϵ 通常不需要调试。

## 学习率衰减

如果设置一个固定的学习率 α，在最小值点附近，由于不同的 batch 中存在一定的噪声，因此不会精确收敛，而是始终在最小值周围一个较大的范围内波动。

而如果随着时间慢慢减少学习率 α 的大小，在初期 α 较大时，下降的步长较大，能以较快的速度进行梯度下降；而后期逐步减小 α 的值，即减小步长，有助于算法的收敛，更容易接近最优解。

最常用的学习率衰减方法：

$$
\alpha = \frac{1}{1 + \text{decay_rate }\times \text{epoch_num} }\times \alpha_0
$$
其中，`decay_rate`为衰减率（超参数），`epoch_num`为将所有的训练样本完整过一遍的次数。

* 指数衰减：

$$
\alpha = 0.95^{\text{epoch_num}} \times \alpha_0
$$

* 其他：

$$
\alpha = \frac{k}{\sqrt{\text{epoch_num}}} \times \alpha_0
$$

$$
\alpha = \frac{k}{\sqrt t}\times \alpha_0
$$



* 离散下降：

![image-20210713102530862](https://gitee.com/xrandx/blog-figurebed/raw/master/img/20210713102530.png)

* 手动调参：对于较小的模型，也有人会在训练时根据进度手动调小学习率。

## 局部最优问题

![saddle](https://gitee.com/xrandx/blog-figurebed/raw/master/img/20210713103016.png)

鞍点（saddle）是函数上的导数为零，但不是轴上局部极值的点。当我们建立一个神经网络时，**通常梯度为零的点是上图所示的鞍点，而非局部最小值。**减少损失的难度也来自误差曲面中的鞍点，而不是局部最低点。因为在一个具有高维度空间的成本函数中，如果梯度为 0，那么在每个方向，成本函数或是凸函数，或是凹函数。而所有维度均需要是凹函数的概率极小，因此在低维度的局部最优点的情况并不适用于高维度。

结论：

* 在训练较大的神经网络、存在大量参数，并且成本函数被定义在较高的维度空间时，困在极差的局部最优中是不大可能的；
* 鞍点附近的平稳段会使得学习非常缓慢，而这也是动量梯度下降法、RMSProp 以及 Adam 优化算法能够加速学习的原因，它们能帮助尽早走出平稳段。

## 参考

吴恩达，深度学习课程