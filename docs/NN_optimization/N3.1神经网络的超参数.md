# 3.1 神经网络的超参数

## 超参调整技巧

### 超参数重要程度排序

目前已经讲到过的超参数中，重要程度依次是（仅供参考）：

* 最重要：
  * 学习率 α

* 其次重要：
  * $β$：动量衰减参数，常设置为 0.9
  * hidden units：各隐藏层神经元个数
  * mini-batch 的大小

* 次要：
  * $β_1, β_2, ϵ$：Adam 优化算法的超参数，常设为 0.9、0.999、$10^{-8}$
  * layers：神经网络层数
  * decay_rate：学习衰减率

### 调参原则

系统组织超参调试过程的技巧：

- 在参数很少的时候可以使用 grid 网格化调参。

* 在更多时候，推荐**随机选点**（不要使用 grid），用这些点实验超参数的效果。这样做的原因是我们很难提前知道超参数的重要程度，随机选点可以通过选择更多值来进行更多实验；
* 由粗糙到精细：当你发现效果不错的点时，聚焦效果不错的点组成的小区域，在其中更密集地随机取值，来搜索参数；

<img src="https://gitee.com/xrandx/blog-figurebed/raw/master/img/20210714192557.png" alt="image-20210714192557380" style="zoom:67%;" />

### 选择合适的范围

* 对于学习率 α，用**对数标尺**而非线性轴更加合理：0.0001、0.001、0.01、0.1 等，然后在这些刻度之间再随机均匀取值；
* 对于 β，取 0.9 就相当于在 10 个值中计算平均值，而取 0.999 就相当于在 1000 个值中计算平均值。可以考虑给 1-β 取值，这样就和取学习率类似了。

上述操作的原因是当 $\beta$ 接近 1 时，即使 $β$ 只有微小的改变，所得结果的灵敏度会有较大的变化。例如，β 从 0.9 增加到 0.9005 对结果（1/(1-β)）几乎没有影响，而 β 从 0.999​ 到 0.9995​ 对结果的影响巨大（从 1000 个值中计算平均值变为 2000 个值中计算平均值）。

