# 3.2 Batch Normalization

批标准化（Batch Normalization，经常简称为 BN）会使参数搜索问题变得很容易，使神经网络对超参数的选择更加稳定，超参数的范围会更庞大，工作效果也很好，也会使训练更容易。

之前，我们对输入特征 X 使用了标准化处理。我们也可以用同样的思路处理隐藏层的激活值 $a^{[l]}$，以加速 $W^{[l+1]}$和 $b^{[l+1]}$的训练。在**实践**中，经常选择标准化 $Z^{[l]}$：

$$
\mu = \frac{1}{m} \sum_i z^{(i)} \\
\sigma^2 = \frac{1}{m} \sum_i {(z_i - \mu)}^2\\
z_{norm}^{(i)} = \frac{z^{(i)} - \mu}{\sqrt{\sigma^2 + \epsilon}}
$$


其中，m 是单个 mini-batch 所包含的样本个数，ϵ 是为了防止分母为零，通常取 $10^{-8}$。

这样，我们使得所有的输入 $z^{(i)}$均值为 0，方差为 1。但我们不想让隐藏层单元总是含有平均值 0 和方差 1，也许隐藏层单元有了不同的分布会更有意义。因此，我们计算

$$
\tilde z^{(i)} = \gamma z^{(i)}_{norm} + \beta
$$
其中，γ 和 β 都是模型的学习参数，所以可以用各种梯度下降算法来更新 γ 和 β 的值，如同更新神经网络的权重一样。

通过对 γ 和 β 的合理设置，可以让 $\tilde z^{(i)}$的均值和方差为任意值。这样，我们对隐藏层的 $z^{(i)}$ 进行标准化处理，用得到的 $\tilde z^{(i)}$替代 $z^{(i)}$。

设置 γ 和 β 的原因是，如果各隐藏层的输入均值在靠近 0 的区域，即处于激活函数的线性区域，不利于训练非线性神经网络，从而得到效果较差的模型。因此，需要用 γ 和 β 对标准化后的结果做进一步处理。

## BN 层为什么有效？

BN层的有效性已有目共睹，但为什么有效可能还需要进一步研究，这里有一些解释，

- **BN层让损失函数更平滑**。论文[**How Does Batch Normalization Help Optimization**](https://arxiv.org/abs/1805.11604)中，通过分析训练过程中每步梯度方向上步长变化引起的损失变化范围、梯度幅值的变化范围、光滑度的变化，认为添**加BN层后，损失函数的 landscape(loss surface)变得更平滑，相比高低不平上下起伏的loss surface，平滑loss surface的梯度预测性更好，可以选取较大的步长。**如下图所示，

  [![https://arxiv.org/abs/1805.11604](https://s2.ax1x.com/2019/12/04/Q16lhn.png)](https://s2.ax1x.com/2019/12/04/Q16lhn.png)

- **BN更有利于梯度下降**。论文[**An empirical analysis of the optimization of deep network loss surfaces**](https://arxiv.org/abs/1612.04010)中，绘制了VGG和NIN网络在有无BN层的情况下，loss surface的差异，包含初始点位置以及不同优化算法最终收敛到的local minima位置，如下图所示。**没有BN层的，其loss surface存在较大的高原，有BN层的则没有高原，而是山峰，因此更容易下降。**

  [![https://arxiv.org/abs/1612.04010](https://s2.ax1x.com/2019/12/04/Q1cAUJ.png)](https://s2.ax1x.com/2019/12/04/Q1cAUJ.png)

- 这里再提供一个直觉上的理解，没有BN层的情况下，网络没办法直接控制每层输入的分布，其分布前面层的权重共同决定，或者说分布的均值和方差“隐藏”在前面层的每个权重中，网络若想调整其分布，需要通过复杂的反向传播过程调整前面的每个权重实现，**BN层的存在相当于将分布的均值和方差从权重中剥离了出来，只需调整γγ和ββ两个参数就可以直接调整分布，让分布和权重的配合变得更加容易。**

这里多说一句，论文[**How Does Batch Normalization Help Optimization**](https://arxiv.org/abs/1805.11604)中对比了标准VGG以及加了BN层的VGG每层分布随训练过程的变化，发现两者并无明显差异，认为BatchNorm并没有改善 **Internal Covariate Shift**。**但这里有个问题是**，两者的训练都可以收敛，对于不能收敛或者训练过程十分震荡的模型呢，其分布变化是怎样的？我也不知道，没做过实验（微笑）。



## Batch Normalization 的作用

使用Batch Normalization，可以获得如下好处，

- **可以使用更大的学习率**，训练过程更加稳定，极大提高了训练速度。
- **可以将bias置为0**，因为 Batch Normalization 的 Standardization 过程会移除直流分量，所以不再需要bias。
- **对权重初始化不再敏感**，通常权重采样自0均值某方差的高斯分布，以往对高斯分布的方差设置十分重要，有了Batch Normalization后，对与同一个输出节点相连的权重进行放缩，其标准差σσ也会放缩同样的倍数，相除抵消。
- **对权重的尺度不再敏感**，理由同上，尺度统一由γγ参数控制，在训练中决定。
- **深层网络可以使用sigmoid和tanh了**，理由同上，BN抑制了梯度消失。
- **Batch Normalization具有某种正则作用，不需要太依赖dropout，减少过拟合**。

> [Batch Normalization详解 - shine-lee - 博客园](https://www.cnblogs.com/shine-lee/p/11989612.html#bn%E5%B1%82%E4%B8%BA%E4%BB%80%E4%B9%88%E6%9C%89%E6%95%88%EF%BC%9F)

