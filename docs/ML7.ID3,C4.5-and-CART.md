# ID3, C4.5 和 CART 算法

<img src="https://gitee.com/xrandx/blog-figurebed/raw/master/img/20210328170928.svg" alt="决策树" style="zoom: 67%;" />

## ID3 算法

信息熵
$$
\mathrm{H}(X) = \sum_i^np_ilog_2(1/p_i) = -\sum_i^np_ilog_2(p_i)
$$
条件熵
$$
\begin{aligned}

H(Y \mid X) 
&=\sum_{i=1}^{n} p\left(x_{i}\right) H\left(Y \mid X=x_{i}\right) \\
&=-\sum_{i=1}^{n} p\left(x_{i}\right) \sum_{j=1}^{m} p\left(y_{j} \mid x_{i}\right) \log _{2} p\left(y_{j} \mid x_{i}\right) \\
&=-\sum_{i=1}^{n} \sum_{j=1}^{m} p\left(x_{i}, y_{j}\right) \log _{2} p\left(y_{j} \mid x_{i}\right)

\end{aligned}
$$
信息增益
$$
G(X) = H(Y) - H(Y|X)
$$


$Y = 1$ 表示买了。$Y = 0$ 表示没买。

$X = 1$ 表示附近学校好。$X = 0$ 表示附近学校不好。

觉得附近学校好，其中买的人有 5 个，不买的个数为 6 个；

觉得附近学校不好的，其中买的人有 1 个，不买的个数为 8 个；

可以得到概率：
$$
P(Y = 1|X = 1) = \frac{5}{11}\\
P(Y = 0|X = 1) = \frac{6}{11} \\
P(Y = 0 | X = 0) = \frac{8}{9} \\
P(Y = 1 | X = 1) = \frac{1}{9} \\
$$

各个条件熵：
$$
\begin{aligned}
H(Y=1|X = 1)  &= -\frac{5}{11}log_2(\frac{5}{11}) -\frac{6}{11}log_2(\frac{6}{11}) = 0.99 
\\
H(Y=1|X=0) &=  -\frac{1}{9}log_2(\frac{1}{9}) -\frac{8}{9}log_2(\frac{8}{9}) = 0.5\\
\end{aligned}
$$
按期望平均得到条件熵，算出 Y = 1 的信息熵：
$$
\begin{aligned}

&P(X = 1) =   \frac{11}{20}\\
&P(X =0) = \frac{9}{20}
\\ \\
&H(Y=1|X) =   \frac{11}{20} \times  0.99  +  \frac{9}{20} \times 0.5 = 0.77
\end{aligned}
$$

算出 Y = 1 的信息熵
$$
H(Y = 1) = -\frac{6}{20}log_2(\frac{6}{20}) -\frac{14}{20}log_2(\frac{14}{20}) = 0.88
$$
然后得出 X 事件的信息增益：
$$
G(X) = H(Y = 1) - H(Y = 1|X) = 0.88-0.77 = 0.11
$$

### 利用信息增益构建决策树

(案例出自西瓜书)

拿西瓜来说，他的样本属性可能是 $[色泽，瓜蒂，敲声，纹理,\dots]$，例如西瓜样本 

<img src="https://gitee.com/xrandx/blog-figurebed/raw/master/img/20210411161337.png" alt="{3C0EB52A-E0E3-4D52-9B78-D62220062A5C}" style="zoom: 67%;" />

我们算出来所有属性的信息增益，D 是样本集合（如上图）：
$$
G(D，瓜蒂) = 0.143 \\
G(D，纹理) = 0.381 \\
G(D，脐部) = 0.289 \\
G(D，触感) = 0.006 \\
G(D，敲声) = 0.141
$$
此时，触感的信息增益最大，我们按照触感划分样本集合，得 D1, D2,  D3

<img src="https://gitee.com/xrandx/blog-figurebed/raw/master/img/20210411162149.png" style="zoom:50%;" />
$$
G(D_1 ， 色泽) = 0.043\\ G(D_1 ，根蒂) = 0.458 \\ G(D_1 ，敲声) = 0.331 \\ G(D_1 ，脐部) = 0.458\\ G(D_1 ，触感) = 0.458
$$
……按照这种划分，我们就建立起了一棵决策树：

<img src="https://gitee.com/xrandx/blog-figurebed/raw/master/img/20210411162915.png" alt="{02BA7EDD-3E25-481F-82EE-52CBA23D1367}" style="zoom: 67%;" />

ID3 算法缺点：

1. 连续特征无法在ID3运用。
2. ID3 采用信息增益大的特征优先建立决策树的节点，在相同条件下，取值比较多的特征比取值少的特征信息增益大，这对预测性能影响很大。
3. ID3算法对于缺失值的情况没有做考虑。
4. 没有考虑过拟合的问题。

后面我们根据这三个问题逐一解决。

## C4.5 算法

### 信息增益比

信息增益准则对取值数目较多的属性有所偏好，ID3 算法的作者 Quinlan 基于上述不足，对ID3算法做了改进，不直接使用信息增益，而使用信息增益比：
$$
R_G(D, A) = \frac{G(D, A)}{IV_A(D)}
$$
D 是样本集合，A 是样本的某个属性，分母是样本 D 关于的属性 A 的固有值 (Intrinsic Value)：
$$
IV_D(A) = -\sum^n_i \frac{|D_i|}{|D|} log_2\frac{|D_i|}{|D|}
$$
属性 A 的某个取值越多，IV 的值就越大：
$$
IV_D(触感) = 0.874 (V = 2) \\ IV_D(色泽) = 1.580 (V = 3) \\ IV_D(编号) = 4.088 (V = 17)
$$
### 连续特征离散化

假设属性 A 的所有取值有 m 个，从小到大排列为 $a_1,a_2,...,a_m$ ，则 C4.5 取相邻两样本值的平均数，一共取得 $m-1$ 个划分点。对于这 $m−1$ 个点，分别计算以该点作为二元分类点时的信息增益。选择信息增益最大的点作为该连续特征的二元离散分类点。

比如取到的增益最大的点为 $a_t$ ,则小于 $a_t$ 的值为 $T_0$ 类别，大于 $a_t$ 的值为 $T_1$ 类别，这样我们就做到了连续特征的离散化。要注意的是，与离散属性不同的是，如果当前节点为连续属性，则该属性后面还可以参与子节点的产生选择过程。

具体来说，假设西瓜数据集有一个颜色深度属性，是被放缩到 [0, 1] 之间的连续值。

| 坏瓜 | 坏瓜 | 好瓜 | 好瓜 | 好瓜 | 坏瓜 | 好瓜 |
| ---- | ---- | ---- | :--- | ---- | ---- | ---- |
| 0.56 | 0.59 | 0.66 | 0.68 | 0.71 | 0.81 | 0.9  |

现在有 7 个数据。先计算相邻两样本值的平均数：

| at1   | at2   | at3  | at4   | at5  | at6   |
| ----- | ----- | ---- | ----- | ---- | ----- |
| 0.575 | 0.625 | 0.67 | 0.695 | 0.72 | 0.855 |

要在这些二元离散分类点找到增益最大的。为啥这里不需要信息增益比？因为所有二元分类点的属性都只有 $T_0$ 和 $T_1$ 。以 $a_{t3}$ 为例子，大于它的值好瓜有 3 个，坏瓜 1 个，小于它的值好瓜有 1 个，坏瓜有 2 个。下式假设 Y 事件为好瓜，$a_{t3}$ 事件为该点为分类点。
$$
\begin{aligned}
H(Y) &= -\frac{4}{7}log_2(\frac{4}{7})-\frac{3}{7}log_2(\frac{3}{7}) = 0.98\\
H(Y | T_1) &= -\frac{1}{4}log_2(\frac{1}{4})-\frac{3}{4}log_2(\frac{3}{4}) = 0.81 \\
H(Y | T_0) &= -\frac{1}{3}log_2(\frac{1}{3})-\frac{2}{3}log_2(\frac{2}{3}) =  0.91 \\
\\
G(a_{t3}) &= H(Y)  - [\frac{4}{7} H(Y | T_0) + \frac{3}{7} H(Y | T_1)] \\
&= 0.98 - [\frac{4}{7} H(Y | T_0) + \frac{3}{7} H(Y | T_1)]  \\
&= 0.12 \\
\end{aligned}
$$
找到最大增益值就可以确定分类点 $a_t$，根据它确定分支节点。

### 缺失值处理

为解决缺失值问题。

1. 在样本某些特征缺失的情况下选择划分的属性
2. 选定了划分属性，对于在该属性上缺失特征的样本的处理





```python
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier


def main():
    iris = load_iris()
    print(iris["feature_names"])

    X = iris.data[:, 2:]  # petal length and width
    y = iris.target

    tree_clf = DecisionTreeClassifier()
    tree_clf.fit(X, y)


if __name__ == '__main__':
    main()
```

### 剪枝增强泛化

决策树和很多算法一样也会出现过拟合现象。我们可以通过剪枝来增强泛化能力。

- 预剪枝（prepruning）：在构造的过程中先评估，再考虑是否分支。

- 后剪枝（post-pruning）：在构造好一颗完整的决策树后，自底向上，评估分支的必要性。

评估指的是性能度量，即决策树的泛化性能。

#### 预剪枝

之前我们就讨论过，将原本的训练集划进一步划分成训练集和验证集。

预剪枝意味着，构造树分支之前，利用验证集，计算决策树不分支时在测试集上的性能，以及分支时的性能。若分支后性能没提升，则选择不分支（即剪枝）。

<img src="https://gitee.com/xrandx/blog-figurebed/raw/master/img/20210418150028.png" alt="2021-04-18_14-59-35" style="zoom:67%;" />

基于上面的训练集(双线上部)与验证集(双线下部)，来试着剪枝：

在划分前，我们取得不考虑这个属性（脐部），视为叶节点（分类结果）为好瓜，则划分精准度为 $\frac{3}{7}=42.9\% $ 。

当我们按照算法算出来信息增益比，根据脐部将训练集按属性分为 3 种取值（凹陷、稍微凹陷、平坦），形成单节点树。对于每个取值，若好瓜比例大，就确定是分类结果（叶节点）是好瓜，反之即坏瓜。再按照这样的分支来测试性能，验证集经过划分后 ，分别分入凹陷、稍微凹陷、平坦三个分支，计算其精度为 $71.4\%$ 。

显然，分支后精度更高，保留此分支，接着利用训练集训练。

![image-20210418150119478](https://gitee.com/xrandx/blog-figurebed/raw/master/img/20210418150119.png)

预剪枝降低了计算时间，减少了过拟合风险。

但预剪枝基于"贪心"本质禁止这些分支展开给预剪枝决策树带来了欠拟含的风险。

#### 后剪枝

后剪枝则表示在构造好一颗完整的决策树后，从最下面的节点开始，考虑该节点分支对模型的性能是否有提升，若无则剪枝，即将该节点标记为叶子节点，类别标记为其包含样本最多的类别。

<img src="https://gitee.com/xrandx/blog-figurebed/raw/master/img/20210418152239.png" alt="2021-04-18_15-22-24" style="zoom:67%;" />

对于已经生成的决策树（如上图），用验证集计算可计算出精度是 42.9% 。

若将最底层分支（纹理）删除替换为叶节点，替换后的叶节点包含编号为 {7, 15} 的**训练样本**，于是该叶节点的类别标记为"好瓜"。决策树在修改后在**验证集**的精度变成了 57.1% ，决定剪枝。

以此类推，若精度提高则剪枝，若相等则随意，若降低则不剪枝。

## CART

### 基尼系数 





### 本文资料参考

[决策树（Decision Tree）-ID3、C4.5、CART比较](https://www.cnblogs.com/huangyc/p/9768858.html)

《机器学习》周志华

