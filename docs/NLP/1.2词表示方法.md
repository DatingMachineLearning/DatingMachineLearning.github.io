# 词表示方法

光是把文本转换为 token 序列是不够的，计算机需要将字符串变成数字才行，这些数字最好能够表达字词之间的关系，这样才容易让机器学习发现其中的规律。

如何表示一个词语的含义？对于语言我们有几种表示的方法：

## One-Hot 编码

有两个句子：

```bash
Time flies like an arrow.
Fruit flies like a banana.
```

对句子进行标记，忽略标点符号，并将所有的单词都用小写字母表示，就会得到一个大小为 8 的词汇表:`{ arrow  banana  like  an  a  flies  fruit  time}`。将他们看成是多标签的 One-Hot 编码：

```python
import pandas as pd
corpus = ['Time flies flies like an arrow.',
          'Fruit flies like a banana.']

result = "".join(corpus)
result = result.replace(".", " ").lower()
result = list(set(result.split()))
result.sort()
dic = {}
for i in range(len(result)):
    dic[result[i]] = [0 for _ in range(len(result))]
    dic[result[i]][i] = 1
# print(dic)
print(pd.DataFrame(dic))
```
```python
# 结果是
a  an  arrow  banana  flies  fruit  like  time
0  1   0      0       0      0      0     0     0
1  0   1      0       0      0      0     0     0
2  0   0      1       0      0      0     0     0
3  0   0      0       1      0      0     0     0
4  0   0      0       0      1      0     0     0
5  0   0      0       0      0      1     0     0
6  0   0      0       0      0      0     1     0
7  0   0      0       0      0      0     0     1
```

这样的表述方法问题很多，比如：

- 所有向量是正交的，没有关于相似性概念。
- 向量维度过大，浪费计算机 CPU。
- 发现句子中的 flies 是不同的意思，这怎么办？

术语频率（TF）和术语频率反转文档频率（TF-IDF）也是 NLP 历史上应用广泛的表示方法：

## TF 表示方法

> [文本向量表示之TFIDF - 知乎](https://zhuanlan.zhihu.com/p/145240120)

> 话说小明今天要做一个文本分类的任务，要将数据集分成军事、科技、体育等类别。
>
> 于是他先对数据集进行了分析，发现，军事里边的文本中包含很多军事相关的词汇，比如“军队”，“运输机”等词，而体育新闻中包含很多“篮球”，“NBA”等词，于是小明思考：**“是否一个词在文本中的词频越高，这个词的权重也就越高呢？**”。

TF（term frequency）表示词频，**即一个文档中，词频越高的词权重越大。**我们想着能不能用词频来表示一个句子：
$$
{\rm TF}_{i, j} = \frac{n_{i, j}}{\sum_{j=0}^k n_{i, j}}
$$
$n_{i,j}$​ 是该词 $t_i$​ 在文档 $d_j$​ 中出现的次数。

```python
from sklearn.feature_extraction.text import CountVectorizer
import matplotlib.pyplot as plt
import seaborn as sns
corpus = ['Time flies flies like an arrow.',
          'Fruit flies like a banana.']
vectorizer = CountVectorizer(binary=False)
X = vectorizer.fit_transform(corpus)
one_hot = X.toarray()
sns.heatmap(one_hot, annot=True,
            cbar=False, xticklabels="vocab",
            yticklabels=['Sentence 1', 'Sentence 2'])
print(vectorizer.get_feature_names())
print(one_hot)
plt.show()
```

输出

```python
['an', 'arrow', 'banana', 'flies', 'fruit', 'like', 'time']
[[1 1 0 2 0 1 1]
 [0 0 1 1 1 1 0]]
```

发现单词 `a` 不见了，它是一个停用词。

停用词是指在信息检索中，为节省存储空间和提高搜索效率，在处理自然语言数据（或文本）之前或之后会自动过滤掉某些字或词，这些字或词即被称为Stop Words（停用词）。 这些停用词都是人工输入、非自动化生成的，生成后的停用词会形成一个停用词表。 但是，并没有一个明确的停用词表能够适用于所有的工具。

>一个中文停用词表：[goto456/stopwords: 中文常用停用词表（哈工大停用词表、百度停用词表等）](https://github.com/goto456/stopwords)
>
>[6.2. Feature extraction — scikit-learn 0.24.2 documentation](https://scikit-learn.org/stable/modules/feature_extraction.html#stop-words)



<img src="C:\Users\Administrator\Desktop\NLP 笔记\img\myplot2.png" alt="myplot2" style="zoom:80%;" />

这是按照词表得到的累计词频向量。

一般来说我们会将其改为 `CountVectorizer(binary=True)`， 也就是词频向量的二进制表示，将频率超过 1 的改为 1，得到：

```python
[[1 1 0 1 0 1 1]
 [0 0 1 1 1 1 0]]
```

![myplot](C:\Users\Administrator\Desktop\NLP 笔记\img\myplot.png)

## TF-IDF 表示

> 但是后来在小明看到文本中的“的”，“了”这些词后就改变了想法，这些词的词频也很高，光词频高还不能体现一个词的权重，怎么办呢，小明发现，“的”，“了”这些词不光在一个文本中的词频比较高，这些词几乎在每个文本中出现过，于是小明又想：“**一个词在越少的文档中出现，那么这个词的权重是否越高呢？**”
>
> 小明灵机一动，既然一个词在文档中出现的频率越高，权重越大，并且包含该词的文档数越少，该词的权重越高，那么能都用这两个指标来综合表示一个词的权重呢？

这就是TF-IDF。

IDF (Inverse Document Frequency) 表示逆文档频率。即一个词出现的文档数越多，这个词的权重越低。


$$
\mathrm{IDF}_i = \log \frac{|D|}{|\{ j : t_ i\in d_j\}|}
$$
$|D|$ 是语料库的文本个数，${\{ j : t_ i\in d_j\}}$ 意味着该词 $t_i$ 存在于该文档 $d_j$ 中，如果该词语不在语料库中，就会导致被除数为零，因此一般情况下使用 ${|\{ j : t_ i\in d_j\}|} + 1$​ 。（参考 [TF-IDF及其算法 - as_ - 博客园](https://www.cnblogs.com/biyeymyhjob/archive/2012/07/17/2595249.HTML)）

关键词 $t_i$ 在文档 $d_j$​ 的TF-IDF值为：
$$
{\rm TF-IDF}_{t_i,d_j} ={\rm TF}_{i, j} \times \mathrm{IDF}_i
$$

> [TF IDF | TFIDF Python Example. An example of how to implement TFIDF… | by Cory Maklin | Towards Data Science](https://towardsdatascience.com/natural-language-processing-feature-engineering-using-tf-idf-e8b9d00e7e76)

代码如下：

```python
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
corpus = ['the man went out for a walk', 'the children sat around the fire']
vectorizer = TfidfVectorizer()
vectors = vectorizer.fit_transform(corpus)
feature_names = vectorizer.get_feature_names()
dense = vectors.todense()
denselist = dense.tolist()
df = pd.DataFrame(denselist, columns=feature_names)
print(df)
```

结果：

```
     around  children      fire      for  ...       sat       the     walk     went
0  0.000000  0.000000  0.000000  0.42616  ...  0.000000  0.303216  0.42616  0.42616
1  0.407401  0.407401  0.407401  0.00000  ...  0.407401  0.579739  0.00000  0.00000
```

