[xmu-xiaoma666/External-Attention-pytorch: 🍀 Pytorch implementation of various Attention Mechanisms, MLP, Re-parameter, Convolution, which is helpful to further understand papers.⭐⭐⭐](https://github.com/xmu-xiaoma666/External-Attention-pytorch)

[MuQiuJun-AI/bert4pytorch: 超轻量级bert的pytorch版本，大量中文注释，容易修改结构，持续更新](https://github.com/MuQiuJun-AI/bert4pytorch?continueFlag=e9813f68c7afd52a2d0bc48ec4ad1ab1)

[提高科研论文写作效率的小工具 - 知乎](https://zhuanlan.zhihu.com/p/34838403)



## 书

[预训练语言模型 (豆瓣)](https://book.douban.com/subject/35458428/)

[基于深度学习的自然语言处理 (豆瓣)](https://book.douban.com/subject/30236842/)

[NLP简介 - Science is interesting.](https://looperxx.github.io/NLP%E7%9A%84%E5%B7%A8%E4%BA%BA%E8%82%A9%E8%86%80/#46-bert)



## Papers

[thunlp/PLMpapers: Must-read Papers on pre-trained language models.](https://github.com/thunlp/PLMpapers)

[datawhalechina/learn-nlp-with-transformers: we want to create a repo to illustrate usage of transformers in chinese](https://github.com/datawhalechina/learn-nlp-with-transformers)


[datawhalechina/daily-interview: Datawhale成员整理的面经，内容包括机器学习，CV，NLP，推荐，开发等，欢迎大家star](https://github.com/datawhalechina/daily-interview)





https://suziwen.github.io/acorns/tutorial/build-your-own-webdav






我经历了NLP的纯自学过程，我个人觉得算是比较快捷的方法。

其中，也把中文资料和斯坦福的CS224N课程相结合，对NLP做了一个入门学习和简单深入，推荐看看我写的这篇总结文章



## Re-invent Bert系列

| 年份 | 论文标题                                                     | 关键词                                                       |
| ---- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 2014 | Memory Network                                               | Memory                                                       |
| 2015 | Neural Machine Translation by Jointly Learning to Align and  Translate Attention | Attention                                                    |
| 2015 | Effective Approaches to Attention-based Neural Machine  Translation | use LSTM                                                     |
| 2015 | End-To-End     Memory Networks                               | multi step attention(k-hop as RNN, global memory)            |
| 2016 | Google’s     Neural Machine Translation System               | low-precision, wordpieces                                    |
| 2016 | A Convolutional Encoder Model for Neural     Machine Translation | CNN encoder                                                  |
| 2017 | Language Modeling with Gated Convolutional Networks          | CNN & GLU(compared with self-attention)                      |
| 2017 | Convolutional     Sequence to Sequence Learning              | Multiple stack, parallel                                     |
| 2017 | Attention is all you need                                    | http://jalammar.github.io/illustrated-transformer/     http://nlp.seas.harvard.edu/2018/04/03/attention.html |
| 2018 | Deep contextualized word representations                     | ELMo                                                         |
| 2018 | Improving Language Understanding     by Generative Pre-Training | GPT                                                          |
| 2018 | BERT: Pre-training of Deep Bidirectional Transformers for Language  Understanding | Bert                                                         |
| 2018 | Language Models are Unsupervised Multitask Learners          | GPT-2                                                        |
| 2019 | RoBERTa: A Robustly Optimized BERT Pretraining Approach      | RoBERTa                                                      |
|      |                                                              |                                                              |
|      |                                                              |                                                              |
|      |                                                              |                                                              |
|      |                                                              |                                                              |

