[xmu-xiaoma666/External-Attention-pytorch: ğŸ€ Pytorch implementation of various Attention Mechanisms, MLP, Re-parameter, Convolution, which is helpful to further understand papers.â­â­â­](https://github.com/xmu-xiaoma666/External-Attention-pytorch)

[MuQiuJun-AI/bert4pytorch: è¶…è½»é‡çº§bertçš„pytorchç‰ˆæœ¬ï¼Œå¤§é‡ä¸­æ–‡æ³¨é‡Šï¼Œå®¹æ˜“ä¿®æ”¹ç»“æ„ï¼ŒæŒç»­æ›´æ–°](https://github.com/MuQiuJun-AI/bert4pytorch?continueFlag=e9813f68c7afd52a2d0bc48ec4ad1ab1)

[æé«˜ç§‘ç ”è®ºæ–‡å†™ä½œæ•ˆç‡çš„å°å·¥å…· - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/34838403)



## ä¹¦

[é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ (è±†ç“£)](https://book.douban.com/subject/35458428/)

[åŸºäºæ·±åº¦å­¦ä¹ çš„è‡ªç„¶è¯­è¨€å¤„ç† (è±†ç“£)](https://book.douban.com/subject/30236842/)

[NLPç®€ä»‹ - Science is interesting.](https://looperxx.github.io/NLP%E7%9A%84%E5%B7%A8%E4%BA%BA%E8%82%A9%E8%86%80/#46-bert)



## Papers

[thunlp/PLMpapers: Must-read Papers on pre-trained language models.](https://github.com/thunlp/PLMpapers)

[datawhalechina/learn-nlp-with-transformers: we want to create a repo to illustrate usage of transformers in chinese](https://github.com/datawhalechina/learn-nlp-with-transformers)


[datawhalechina/daily-interview: Datawhaleæˆå‘˜æ•´ç†çš„é¢ç»ï¼Œå†…å®¹åŒ…æ‹¬æœºå™¨å­¦ä¹ ï¼ŒCVï¼ŒNLPï¼Œæ¨èï¼Œå¼€å‘ç­‰ï¼Œæ¬¢è¿å¤§å®¶star](https://github.com/datawhalechina/daily-interview)





https://suziwen.github.io/acorns/tutorial/build-your-own-webdav






æˆ‘ç»å†äº†NLPçš„çº¯è‡ªå­¦è¿‡ç¨‹ï¼Œæˆ‘ä¸ªäººè§‰å¾—ç®—æ˜¯æ¯”è¾ƒå¿«æ·çš„æ–¹æ³•ã€‚

å…¶ä¸­ï¼Œä¹ŸæŠŠä¸­æ–‡èµ„æ–™å’Œæ–¯å¦ç¦çš„CS224Nè¯¾ç¨‹ç›¸ç»“åˆï¼Œå¯¹NLPåšäº†ä¸€ä¸ªå…¥é—¨å­¦ä¹ å’Œç®€å•æ·±å…¥ï¼Œæ¨èçœ‹çœ‹æˆ‘å†™çš„è¿™ç¯‡æ€»ç»“æ–‡ç« 



## Re-invent Bertç³»åˆ—

| å¹´ä»½ | è®ºæ–‡æ ‡é¢˜                                                     | å…³é”®è¯                                                       |
| ---- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 2014 | Memory Network                                               | Memory                                                       |
| 2015 | Neural Machine Translation by Jointly Learning to Align and  Translate Attention | Attention                                                    |
| 2015 | Effective Approaches to Attention-based Neural Machine  Translation | use LSTM                                                     |
| 2015 | End-To-End     Memory Networks                               | multi step attention(k-hop as RNN, global memory)            |
| 2016 | Googleâ€™s     Neural Machine Translation System               | low-precision, wordpieces                                    |
| 2016 | A Convolutional Encoder Model for Neural     Machine Translation | CNN encoder                                                  |
| 2017 | Language Modeling with Gated Convolutional Networks          | CNN & GLU(compared with self-attention)                      |
| 2017 | Convolutional     Sequence to Sequence Learning              | Multiple stack, parallel                                     |
| 2017 | Attention is all you need                                    | http://jalammar.github.io/illustrated-transformer/     http://nlp.seas.harvard.edu/2018/04/03/attention.html |
| 2018 | Deep contextualized word representations                     | ELMo                                                         |
| 2018 | Improving Language Understanding     by Generative Pre-Training | GPT                                                          |
| 2018 | BERT: Pre-training of Deep Bidirectional Transformers for Language  Understanding | Bert                                                         |
| 2018 | Language Models are Unsupervised Multitask Learners          | GPT-2                                                        |
| 2019 | RoBERTa: A Robustly Optimized BERT Pretraining Approach      | RoBERTa                                                      |
|      |                                                              |                                                              |
|      |                                                              |                                                              |
|      |                                                              |                                                              |
|      |                                                              |                                                              |

