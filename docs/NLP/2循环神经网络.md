# 循环神经网络

## 数学符号

对于一个序列数据 $x$，用符号 $x^{⟨t⟩}$来表示这个数据中的第 $t$ 个元素，用 $y^{⟨t⟩}$来表示第 $t$ 个标签，用 $T_x$ 和 $T_y$ 来表示输入和输出的长度。对于一段音频，元素可能是其中的几帧；对于一句话，元素可能是一到多个单词。

我们用 $x^{(i)}$ 来表示第$i$个训练样本，所以为了指代第 $t$ 个元素，或者说是训练样本 $i$ 的序列中第 $t$ 个元素用 $x^{\left(i \right) <t>}$ 这个符号来表示。对应即有 $T^{(i)}_x$ 和 $T^{(i)}_y$。

## 问题

对于序列数据，使用标准神经网络存在以下问题：

* 对于不同的示例，输入和输出可能有不同的长度，因此输入层和输出层的神经元数量无法固定。
* 从输入文本的不同位置学到的同一特征无法共享。
* 模型中的参数太多，计算量太大。

## RNN（循环神经网络）

为了解决这些问题，引入循环神经网络（Recurrent Neural Network，RNN）。一种循环神经网络的结构如下图所示：

![Recurrent-Neural-Network](img\Recurrent-Neural-Network.png)

### 前向传播

![image-20210801154552727](img\image-20210801154552727.png)

当元素 $x^{⟨t⟩}$​ 输入对应时间步（Time Step）的隐藏层的同时，该隐藏层也会接收来自上一时间步的隐藏层的激活值 $a^{⟨t-1⟩}$​，其中 $a^{⟨0⟩}$​ 一般直接初始化为零向量。一个时间步输出一个对应的预测结果 $\hat y^{⟨t⟩}$​。

循环神经网络从左向右扫描数据，同时每个时间步的参数也是共享的，输入、激活、输出的参数对应为 $W_{ax}$、$W_{aa}$、$W_{ya}$。

下图是一个 RNN 神经元的结构：

![RNN-cell](img\RNN-cell.png)

前向传播过程的W公式如下：

$$
\begin{aligned}
a^{⟨0⟩} &= \vec{0} \\
a^{⟨t⟩} &= g_1(W_{aa}a^{⟨t-1⟩} + W_{ax}x^{⟨t⟩} + b_a) \\
\hat y^{⟨t⟩} &= g_2(W_{ya}a^{⟨t⟩} + b_y)
\end{aligned}
$$
激活函数 $g_1$通常选择 tanh，有时也用 ReLU；$g_2$可选 sigmoid 或 softmax，取决于需要的输出类型。

为了进一步简化公式以方便运算，可以将 $W_{aa}$、$W_{ax}$ **水平并列**为一个矩阵 $W_a$，同时 $a^{⟨t-1⟩}$和 $ x^{⟨t⟩}$ **堆叠**成一个矩阵。则有：

$$
\begin{aligned}
W_a &= [W_{aa}, W_{ax}] \\
a^{⟨t⟩} &= g_1(W_a[a^{⟨t-1⟩}; x^{⟨t⟩}] + b_a) \\
\hat y^{⟨t⟩} &= g_2(W_{ya}a^{⟨t⟩} + b_y)
\end{aligned}
$$

### 反向传播

![image-20210801153429336](img\image-20210801152148684.png)

为了计算反向传播过程，需要先定义一个损失函数。单个位置上（或者说单个时间步上）某个单词的预测值的损失函数采用交叉熵损失函数，如下所示：

$$
L^{⟨t⟩}(\hat y^{⟨t⟩}, y^{⟨t⟩}) = -y^{⟨t⟩}log\hat y^{⟨t⟩} - (1 - y^{⟨t⟩})log(1-\hat y^{⟨t⟩})
$$
将单个位置上的损失函数相加，得到整个序列的成本函数如下：

$$
J = L(\hat y, y) = \sum^{T_x}_{t=1} L^{⟨t⟩}(\hat y^{⟨t⟩}, y^{⟨t⟩})
$$
循环神经网络的反向传播被称为**通过时间反向传播（Backpropagation through time）**，因为从右向左计算的过程就像是时间倒流。

更详细的计算公式如下：

![formula-of-RNN](img\formula-of-RNN.png)

### 不同结构

某些情况下，输入长度和输出长度不一致。根据所需的输入及输出长度，循环神经网络可分为“一对一”、“多对一”、“多对多”等结构：

![Examples-of-RNN-architectures](img\Examples-of-RNN-architectures.png)

目前我们看到的模型的问题是，只使用了这个序列中之前的信息来做出预测，即后文没有被使用。可以通过**双向循环神经网络（Bidirectional RNN，BRNN）**来解决这个问题。

![image-20210802195606924](img\image-20210802195606924.png)

建立**RNN**模型，我们继续使用“**Cats average 15 hours of sleep a day.**”这个句子来作为我们的运行样例，我将会画出一个**RNN**结构。在第0个时间步，你要计算激活项$a^{<1>}$，它是以$x^{<1 >}$作为输入的函数，而$x^{<1>}$会被设为全为0的集合，也就是0向量。

然后RNN进入下个时间步，在下一时间步中，仍然使用激活项$a^{<1>}$，在这步要做的是计算出第二个词会是什么。现在我们依然传给它正确的第一个词，我们会告诉它第一个词就是**Cats**，也就是$\hat y^{<1>}$，告诉它第一个词就是**Cats**，这就是为什么$y^{<1>} = x^{<2>}$（上图编号2所示）。

在你训练一个序列模型之后，要想了解到这个模型学到了什么，一种非正式的方法就是进行一次新序列采样，来看看到底应该怎么做。

记住一个序列模型模拟了任意特定单词序列的概率，我们要做的就是对这些概率分布进行采样来生成一个新的单词序列。遇到 **<UNK>**  最好是随机输出一个单词。

![image-20210802195102737](img\image-20210802195102737.png)

## 梯度消失

RNN 有个显著的缺点：不擅长捕获长程依赖。
$$
\begin{aligned}
&\text{The cat, which already ate ……, was full.} \\
&\text{The cats, which ate ……, were full.}
\end{aligned}
$$
RNN 很容易将这两个句子的 was 和 were 弄混，这个例子中的句子有长期的依赖，最前面的单词对句子后面的单词有影响。但是我们目前见到的基本的 RNN 模型，不擅长捕获这种长程依赖效应。因为 RNN 同样也存在梯度消失的问题，由于梯度消失，在反向传播时，后面层的输出误差很难影响到较靠前层的计算，网络很难调整靠前的计算。

在反向传播时，随着层数的增多，梯度不仅可能指数型下降，也有可能指数型上升，即梯度爆炸。不过梯度爆炸比较容易发现，因为参数会急剧膨胀到数值溢出，可能显示为 `NaN`。这时可以采用**梯度裁剪**（Gradient Clipping）来解决：观察梯度向量，如果它大于某个阈值，则缩放梯度向量以保证其不会太大。

相比之下，梯度消失问题更难解决。**GRU 和 LSTM 都可以作为缓解梯度消失问题的方案**。

## GRU（门控循环单元）

![微信截图_20210811164035](img\微信截图_20210811164035.png)

**GRU（Gated Recurrent Units, 门控循环单元）**改善了 RNN 的隐藏层，使其可以更好地**捕捉深层连接**，并改善了梯度消失问题。
$$
\begin{aligned}
\tilde c^{⟨t⟩} &= tanh(W_c[c^{⟨t-1⟩}, x^{⟨t⟩}] + b_c)
\\ \\
Γ_u &= \sigma(W_u[c^{⟨t-1⟩}, x^{⟨t⟩}] + b_u)
\\ \\
c^{⟨t⟩} &= Γ_u \times \tilde c^{⟨t⟩} + (1 - Γ_u) \times c^{⟨t-1⟩}
\\ \\
a^{⟨t⟩} &= c^{⟨t⟩}
\end{aligned}
$$
$\tilde c^{⟨t⟩}$ 就是由上一层输出值 $c^{⟨t-1⟩}$ 计算出的候选值。

当使用 sigmoid 作为激活函数 $\sigma$ 来得到 $Γ_u$时，$Γ_u$ 的值在 0 到 1 的范围内，且大多数时间非常接近于 0 或 1。

当 $Γ_u = 1$时，$c^{⟨t⟩}$被更新为 $\tilde c^{⟨t⟩}$，更新。

当 $Γ_u = 0$时，$c^{⟨t⟩}$ 保持为 $c^{⟨t-1⟩}$，保留。

因为 $Γ_u$ 可以很接近 0，因此 $c^{⟨t⟩}$几乎就等于 $c^{⟨t-1⟩}$。在经过很长的序列后，$c$ 的值依然被维持，从而实现“记忆”的功能。

以上实际上是简化过的 GRU 单元，但是蕴涵了 GRU 最重要的思想。完整的 GRU 单元添加了一个新的**相关门（Relevance Gate）** $Γ_r$，表示 $\tilde c^{⟨t⟩}$和 $c^{⟨t⟩}$的相关性。因此，表达式改为如下所示：
$$
\begin{aligned}
\\ \\ 
\tilde c^{⟨t⟩} &= tanh(W_c[Γ_r \times c^{⟨t-1⟩}, x^{⟨t⟩}] + b_c) 
\\ \\ 
Γ_u &= \sigma(W_u[c^{⟨t-1⟩}, x^{⟨t⟩}] + b_u)
\\ \\ 
Γ_r &= \sigma(W_r[c^{⟨t-1⟩}, x^{⟨t⟩}] + b_r)
\\ \\ 
c^{⟨t⟩} &= Γ_u \times \tilde c^{⟨t⟩} + (1 - Γ_u) \times c^{⟨t-1⟩}
\\ \\ 
a^{⟨t⟩} &= c^{⟨t⟩}
\\ \\ 
\end{aligned}
$$
**GRU 的思路在于传播或更新上一层信息。**

论文：

1. [Cho et al., 2014. On the properties of neural machine translation: Encoder-decoder approaches](https://arxiv.org/pdf/1409.1259.pdf)
2. [Chung et al., 2014. Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](https://arxiv.org/pdf/1412.3555.pdf)



## LSTM（长短期记忆）

<img src="img\image-20210820114505645.png" alt="image-20210820114505645" style="zoom:67%;" />

LSTM（Long Short Term Memory，长短期记忆）网络比 GRU 更加灵活和强大，它额外引入了**遗忘门（Forget Gate）** $Γ_f$和**输出门（Output Gate）** $Γ_o$。用来取代 GRU 的 $\Gamma_{u}$ 和 $1-\Gamma_{u}$。

其结构图如下：

![LSTM](img\LSTM.png)

公式：
$$
\begin{aligned}
\tilde c^{⟨t⟩} &= tanh(W_c[a^{⟨t-1⟩}, x^{⟨t⟩}] + b_c)\\ \\ 
Γ_u &= \sigma(W_u[a^{⟨t-1⟩}, x^{⟨t⟩}] + b_u)\\ \\ 
Γ_f &= \sigma(W_f[a^{⟨t-1⟩}, x^{⟨t⟩}] + b_f)\\ \\ 
Γ_o &= \sigma(W_o[a^{⟨t-1⟩}, x^{⟨t⟩}] + b_o)\\ \\ 
c^{⟨t⟩} &= Γ^{⟨t⟩}_u \times \tilde c^{⟨t⟩} + Γ^{⟨t⟩}_f \times c^{⟨t-1⟩}\\ \\ 
a^{⟨t⟩} &= Γ_o^{⟨t⟩} \times tanh(c^{⟨t⟩})
\end{aligned}
$$
将多个 LSTM 单元按时间次序连接起来，就得到一个 LSTM 网络。

以上是简化版的 LSTM。在更为常用的版本中，几个门值不仅取决于 $a^{⟨t-1⟩}$和 $x^{⟨t⟩}$，有时也可以偷窥上一个记忆细胞输入的值 $c^{⟨t-1⟩}$，这被称为窥视孔连接（Peephole Connection)。但如果你读过论文，见人讨论“**偷窥孔连接**”，那就是在说$c^{<t-1>}$也能影响门值。这时，和 GRU 不同，$c^{⟨t-1⟩}$和门值是一对一的。

$c^{0}$ 常被初始化为零向量。

**LSTM**反向传播计算：

**门求偏导：**

$$
\begin{aligned}
d \Gamma_o^{\langle t \rangle} &= da_{next}*\tanh(c_{next}) * \Gamma_o^{\langle t \rangle}*(1-\Gamma_o^{\langle t \rangle})
\\\\
d\tilde c^{\langle t \rangle} &= dc_{next}*\Gamma_i^{\langle t \rangle}+ \Gamma_o^{\langle t \rangle} (1-\tanh(c_{next})^2) * i_t * da_{next} * \tilde c^{\langle t \rangle} * (1-\tanh(\tilde c)^2)  \\\\

d\Gamma_u^{\langle t \rangle} &= dc_{next}*\tilde c^{\langle t \rangle} + \Gamma_o^{\langle t \rangle} (1-\tanh(c_{next})^2) * \tilde c^{\langle t \rangle} * da_{next}*\Gamma_u^{\langle t \rangle}*(1-\Gamma_u^{\langle t \rangle}) \\\\

d\Gamma_f^{\langle t \rangle} &= dc_{next}*\tilde c_{prev} + \Gamma_o^{\langle t \rangle} (1-\tanh(c_{next})^2) * c_{prev} * da_{next}*\Gamma_f^{\langle t \rangle}*(1-\Gamma_f^{\langle t \rangle})
\end{aligned}
$$
**参数求偏导 ：**
$$
\begin{aligned}
 dW_f &= d\Gamma_f^{\langle t \rangle} * \begin{pmatrix} a_{prev} \\ x_t\end{pmatrix}^T   \\\\
 dW_u &= d\Gamma_u^{\langle t \rangle} * \begin{pmatrix} a_{prev} \\ x_t\end{pmatrix}^T  \\\\
  dW_c &= d\tilde c^{\langle t \rangle} * \begin{pmatrix} a_{prev} \\ x_t\end{pmatrix}^T  \\\\
 dW_o &= d\Gamma_o^{\langle t \rangle} * \begin{pmatrix} a_{prev} \\ x_t\end{pmatrix}^T 
\end{aligned}
$$
为了计算$db_f, db_u, db_c, db_o$ 需要各自对$d\Gamma_f^{\langle t \rangle}, d\Gamma_u^{\langle t \rangle}, d\tilde c^{\langle t \rangle}, d\Gamma_o^{\langle t \rangle}$ 求和。

最后，计算隐藏状态、记忆状态和输入的偏导数：

$$
\begin{aligned}
 da_{prev} &= W_f^T*d\Gamma_f^{\langle t \rangle} + W_u^T * d\Gamma_u^{\langle t \rangle}+ W_c^T * d\tilde c^{\langle t \rangle} + W_o^T * d\Gamma_o^{\langle t \rangle}  \\\\

 dc_{prev} &= dc_{next}\Gamma_f^{\langle t \rangle} + \Gamma_o^{\langle t \rangle} * (1- \tanh(c_{next})^2)*\Gamma_f^{\langle t \rangle}*da_{next}  \\\\
 dx^{\langle t \rangle} &= W_f^T*d\Gamma_f^{\langle t \rangle} + W_u^T * d\Gamma_u^{\langle t \rangle}+ W_c^T * d\tilde c_t + W_o^T * d\Gamma_o^{\langle t \rangle}  
\end{aligned}
$$
这就是LSTM，我们什么时候应该用GRU？什么时候用LSTM？这里没有统一的准则。而且即使我先讲解了GRU，在深度学习的历史上，LSTM也是更早出现的，而GRU是最近才发明出来的，它可能源于Pavia在更加复杂的LSTM模型中做出的简化。研究者们在很多不同问题上尝试了这两种模型，看看在不同的问题不同的算法中哪个模型更好，所以这不是个学术和高深的算法，我才想要把这两个模型展示给你。

GRU的优点是这是个更加简单的模型，所以更容易创建一个更大的网络，而且它只有两个门，在计算性上也运行得更快，然后它可以扩大模型的规模。

但是LSTM更加强大和灵活，因为它有三个门而不是两个。如果你想选一个使用，我认为LSTM在历史进程上是个更优先的选择，所以如果你必须选一个，我感觉今天大部分的人还是会把LSTM作为默认的选择来尝试。虽然我认为最近几年GRU获得了很多支持，而且我感觉越来越多的团队也正在使用GRU，因为它更加简单，而且还效果还不错，它更容易适应规模更加大的问题。

所以这就是LSTM，无论是GRU还是LSTM，你都可以用它们来构建捕获更加深层连接的神经网络。

相关论文：[Hochreiter & Schmidhuber 1997. Long short-term memory](https://www.researchgate.net/publication/13853244_Long_Short-term_Memory)



## BRNN（双向循环神经网络）

单向的循环神经网络在某一时刻的预测结果只能使用之前输入的序列信息。**双向循环神经网络（Bidirectional RNN，BRNN）**可以在序列的任意位置使用之前和之后的数据。其工作原理是增加一个反向循环层，结构如下图所示：

![BRNN](img\BRNN.png)

因此，有
$$
y^{⟨t⟩} = g(W_y[\overrightarrow a^{⟨t⟩},  \overleftarrow a^{⟨t⟩}] + b_y)
$$
这个改进的方法不仅能用于基本的 RNN，也可以用于 GRU 或 LSTM。**缺点是需要完整的序列数据**，才能预测任意位置的结果。例如构建语音识别系统，需要等待用户说完并获取整个语音表达，才能处理这段语音并进一步做语音识别。因此，实际应用会有更加复杂的模块。

![image-20210820151432989](img\image-20210820151432989.png)

## DRNN（深度循环神经网络）

把这里的符号稍微改了一下，不再用原来的 $a^{<0 >}$ 表示 0 时刻的激活值了，而是用 $a^{\lbrack 1\rbrack <0>}$ 来表示第一层（上图编号4所示），所以我们现在用 $a^{\lbrack l\rbrack <t>}$ 来表示第 $l$ 层的激活值，这个 $<t>$ 表示第 $t$ 个时间点，这样就可以表示。第一层第一个时间点的激活值$a^{\lbrack 1\rbrack <1>}$，这（$a^{\lbrack 1\rbrack <2>}$）就是第一层第二个时间点的激活值，$a^{\lbrack 1\rbrack <3>}$和$a^{\lbrack 1\rbrack <4>}$。然后我们把这些（上图编号4方框内所示的部分）堆叠在上面，这就是一个有三个隐层的新的网络。

循环神经网络的每个时间步上也可以包含多个隐藏层，形成**深度循环神经网络（Deep RNN)**。结构如下图所示：

![image-20210820161657261](img\image-20210820161614563.png)

以 $a^{[2]⟨3⟩}$为例，有 $a^{[2]⟨3⟩} = g(W_a^{[2]}[a^{[2]⟨2⟩}, a^{[1]⟨3⟩}] + b_a^{[2]})$。



相关网站：

[Understanding LSTM Networks -- colah's blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)

[LSTM神经网络输入输出究竟是怎样的？ - 知乎](https://www.zhihu.com/question/41949741)

[如何理解LSTM中的time step？ - 知乎](https://www.zhihu.com/question/271774530)

[直觉理解LSTM和GRU - 知乎](https://zhuanlan.zhihu.com/p/37204589)

[深入理解lstm及其变种gru - 知乎](https://zhuanlan.zhihu.com/p/34203833)

[动图详解LSTM和GRU - 知乎](https://zhuanlan.zhihu.com/p/150827731)



[cswangjiawei/pytorch-ChineseNER: 基于BiLSTM-CRF的字级别的中文命名实体识别库](https://github.com/cswangjiawei/pytorch-ChineseNER)
