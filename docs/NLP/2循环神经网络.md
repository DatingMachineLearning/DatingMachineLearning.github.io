# 循环神经网络

## 数学符号

对于一个序列数据 $x$​，用符号 $x^{⟨t⟩}$​来表示这个数据中的第 $t$​ 个元素，用 $y^{⟨t⟩}$​来表示第 $t$​ 个标签，用 $T_x$​ 和 $T_y$​ 来表示输入和输出的长度。对于一段音频，元素可能是其中的几帧；对于一句话，元素可能是一到多个单词。

第 $i$ 个序列数据的第 $t$ 个元素用符号 $x^{(i)⟨t⟩}$，第 $t$ 个标签即为 $y^{(i)⟨t⟩}$。对应即有 $T^{(i)}_x$ 和 $T^{(i)}_y$。

## 问题

对于序列数据，使用标准神经网络存在以下问题：

* 对于不同的示例，输入和输出可能有不同的长度，因此输入层和输出层的神经元数量无法固定。
* 从输入文本的不同位置学到的同一特征无法共享。
* 模型中的参数太多，计算量太大。

## RNN

为了解决这些问题，引入**循环神经网络（Recurrent Neural Network，RNN）**。一种循环神经网络的结构如下图所示：

![Recurrent-Neural-Network](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Sequence_Models/Recurrent-Neural-Network.png)

### 前向传播

当元素 $x^{⟨t⟩}$​ 输入对应时间步（Time Step）的隐藏层的同时，该隐藏层也会接收来自上一时间步的隐藏层的激活值 $a^{⟨t-1⟩}$​，其中 $a^{⟨0⟩}$​ 一般直接初始化为零向量。一个时间步输出一个对应的预测结果 $\hat y^{⟨t⟩}$​。

循环神经网络从左向右扫描数据，同时每个时间步的参数也是共享的，输入、激活、输出的参数对应为 $W_{ax}$、$W_{aa}$、$W_{ya}$。

下图是一个 RNN 神经元的结构：

![RNN-cell](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Sequence_Models/RNN-cell.png)

前向传播过程的W公式如下：

$$
\begin{aligned}
a^{⟨0⟩} &= \vec{0} \\
a^{⟨t⟩} &= g_1(W_{aa}a^{⟨t-1⟩} + W_{ax}x^{⟨t⟩} + b_a) \\
\hat y^{⟨t⟩} &= g_2(W_{ya}a^{⟨t⟩} + b_y)
\end{aligned}
$$
激活函数 $g_1$通常选择 tanh，有时也用 ReLU；$g_2$可选 sigmoid 或 softmax，取决于需要的输出类型。

为了进一步简化公式以方便运算，可以将 $W_{aa}$、$W_{ax}$ **水平并列**为一个矩阵 $W_a$，同时 $a^{⟨t-1⟩}$和 $ x^{⟨t⟩}$ **堆叠**成一个矩阵。则有：

$$
\begin{aligned}
W_a &= [W_{aa}, W_{ax}] \\
a^{⟨t⟩} &= g_1(W_a[a^{⟨t-1⟩}; x^{⟨t⟩}] + b_a) \\
\hat y^{⟨t⟩} &= g_2(W_{ya}a^{⟨t⟩} + b_y)
\end{aligned}
$$

### 反向传播

为了计算反向传播过程，需要先定义一个损失函数。单个位置上（或者说单个时间步上）某个单词的预测值的损失函数采用交叉熵损失函数，如下所示：

$$
L^{⟨t⟩}(\hat y^{⟨t⟩}, y^{⟨t⟩}) = -y^{⟨t⟩}log\hat y^{⟨t⟩} - (1 - y^{⟨t⟩})log(1-\hat y^{⟨t⟩})
$$
将单个位置上的损失函数相加，得到整个序列的成本函数如下：

$$
J = L(\hat y, y) = \sum^{T_x}_{t=1} L^{⟨t⟩}(\hat y^{⟨t⟩}, y^{⟨t⟩})
$$
循环神经网络的反向传播被称为**通过时间反向传播（Backpropagation through time）**，因为从右向左计算的过程就像是时间倒流。

更详细的计算公式如下：

![formula-of-RNN](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Sequence_Models/formula-of-RNN.png)

