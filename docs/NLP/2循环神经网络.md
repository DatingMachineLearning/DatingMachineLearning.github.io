# 循环神经网络

## 数学符号

对于一个序列数据 $x$，用符号 $x^{⟨t⟩}$来表示这个数据中的第 $t$ 个元素，用 $y^{⟨t⟩}$来表示第 $t$ 个标签，用 $T_x$ 和 $T_y$ 来表示输入和输出的长度。对于一段音频，元素可能是其中的几帧；对于一句话，元素可能是一到多个单词。

我们用 $x^{(i)}$ 来表示第$i$个训练样本，所以为了指代第 $t$ 个元素，或者说是训练样本 $i$ 的序列中第 $t$ 个元素用 $x^{\left(i \right) <t>}$ 这个符号来表示。对应即有 $T^{(i)}_x$ 和 $T^{(i)}_y$。

## 问题

对于序列数据，使用标准神经网络存在以下问题：

* 对于不同的示例，输入和输出可能有不同的长度，因此输入层和输出层的神经元数量无法固定。
* 从输入文本的不同位置学到的同一特征无法共享。
* 模型中的参数太多，计算量太大。

## RNN

为了解决这些问题，引入循环神经网络（Recurrent Neural Network，RNN）。一种循环神经网络的结构如下图所示：

![Recurrent-Neural-Network](img\Recurrent-Neural-Network.png)

### 前向传播

![image-20210801154552727](img\image-20210801154552727.png)

当元素 $x^{⟨t⟩}$​ 输入对应时间步（Time Step）的隐藏层的同时，该隐藏层也会接收来自上一时间步的隐藏层的激活值 $a^{⟨t-1⟩}$​，其中 $a^{⟨0⟩}$​ 一般直接初始化为零向量。一个时间步输出一个对应的预测结果 $\hat y^{⟨t⟩}$​。

循环神经网络从左向右扫描数据，同时每个时间步的参数也是共享的，输入、激活、输出的参数对应为 $W_{ax}$、$W_{aa}$、$W_{ya}$。

下图是一个 RNN 神经元的结构：

![RNN-cell](img\RNN-cell.png)

前向传播过程的W公式如下：

$$
\begin{aligned}
a^{⟨0⟩} &= \vec{0} \\
a^{⟨t⟩} &= g_1(W_{aa}a^{⟨t-1⟩} + W_{ax}x^{⟨t⟩} + b_a) \\
\hat y^{⟨t⟩} &= g_2(W_{ya}a^{⟨t⟩} + b_y)
\end{aligned}
$$
激活函数 $g_1$通常选择 tanh，有时也用 ReLU；$g_2$可选 sigmoid 或 softmax，取决于需要的输出类型。

为了进一步简化公式以方便运算，可以将 $W_{aa}$、$W_{ax}$ **水平并列**为一个矩阵 $W_a$，同时 $a^{⟨t-1⟩}$和 $ x^{⟨t⟩}$ **堆叠**成一个矩阵。则有：

$$
\begin{aligned}
W_a &= [W_{aa}, W_{ax}] \\
a^{⟨t⟩} &= g_1(W_a[a^{⟨t-1⟩}; x^{⟨t⟩}] + b_a) \\
\hat y^{⟨t⟩} &= g_2(W_{ya}a^{⟨t⟩} + b_y)
\end{aligned}
$$

### 反向传播

![image-20210801153429336](img\image-20210801152148684.png)

为了计算反向传播过程，需要先定义一个损失函数。单个位置上（或者说单个时间步上）某个单词的预测值的损失函数采用交叉熵损失函数，如下所示：

$$
L^{⟨t⟩}(\hat y^{⟨t⟩}, y^{⟨t⟩}) = -y^{⟨t⟩}log\hat y^{⟨t⟩} - (1 - y^{⟨t⟩})log(1-\hat y^{⟨t⟩})
$$
将单个位置上的损失函数相加，得到整个序列的成本函数如下：

$$
J = L(\hat y, y) = \sum^{T_x}_{t=1} L^{⟨t⟩}(\hat y^{⟨t⟩}, y^{⟨t⟩})
$$
循环神经网络的反向传播被称为**通过时间反向传播（Backpropagation through time）**，因为从右向左计算的过程就像是时间倒流。

更详细的计算公式如下：

![formula-of-RNN](img\formula-of-RNN.png)

### 不同结构

某些情况下，输入长度和输出长度不一致。根据所需的输入及输出长度，循环神经网络可分为“一对一”、“多对一”、“多对多”等结构：

![Examples-of-RNN-architectures](img\Examples-of-RNN-architectures.png)

目前我们看到的模型的问题是，只使用了这个序列中之前的信息来做出预测，即后文没有被使用。可以通过**双向循环神经网络（Bidirectional RNN，BRNN）**来解决这个问题。

![image-20210802195606924](img\image-20210802195606924.png)

建立**RNN**模型，我们继续使用“**Cats average 15 hours of sleep a day.**”这个句子来作为我们的运行样例，我将会画出一个**RNN**结构。在第0个时间步，你要计算激活项$a^{<1>}$，它是以$x^{<1 >}$作为输入的函数，而$x^{<1>}$会被设为全为0的集合，也就是0向量。

然后RNN进入下个时间步，在下一时间步中，仍然使用激活项$a^{<1>}$，在这步要做的是计算出第二个词会是什么。现在我们依然传给它正确的第一个词，我们会告诉它第一个词就是**Cats**，也就是$\hat y^{<1>}$，告诉它第一个词就是**Cats**，这就是为什么$y^{<1>} = x^{<2>}$（上图编号2所示）。

在你训练一个序列模型之后，要想了解到这个模型学到了什么，一种非正式的方法就是进行一次新序列采样，来看看到底应该怎么做。

记住一个序列模型模拟了任意特定单词序列的概率，我们要做的就是对这些概率分布进行采样来生成一个新的单词序列。遇到 **<UNK>**  最好是随机输出一个单词。

![image-20210802195102737](img\image-20210802195102737.png)

## 梯度消失

RNN 有个显著的缺点：不擅长捕获长程依赖。
$$
\begin{aligned}
&\text{The cat, which already ate ……, was full.} \\
&\text{The cats, which ate ……, were full.}
\end{aligned}
$$
RNN 很容易将这两个句子的 was 和 were 弄混，这个例子中的句子有长期的依赖，最前面的单词对句子后面的单词有影响。但是我们目前见到的基本的 RNN 模型，不擅长捕获这种长程依赖效应。因为 RNN 同样也存在梯度消失的问题，由于梯度消失，在反向传播时，后面层的输出误差很难影响到较靠前层的计算，网络很难调整靠前的计算。

在反向传播时，随着层数的增多，梯度不仅可能指数型下降，也有可能指数型上升，即梯度爆炸。不过梯度爆炸比较容易发现，因为参数会急剧膨胀到数值溢出，可能显示为 `NaN`。这时可以采用**梯度裁剪**（Gradient Clipping）来解决：观察梯度向量，如果它大于某个阈值，则缩放梯度向量以保证其不会太大。

相比之下，梯度消失问题更难解决。**GRU 和 LSTM 都可以作为缓解梯度消失问题的方案**。

## GRU（门控循环单元）

![微信截图_20210811164035](img\微信截图_20210811164035.png)

**GRU（Gated Recurrent Units, 门控循环单元）**改善了 RNN 的隐藏层，使其可以更好地**捕捉深层连接**，并改善了梯度消失问题。
$$
\begin{aligned}
\tilde c^{⟨t⟩} &= tanh(W_c[c^{⟨t-1⟩}, x^{⟨t⟩}] + b_c)
\\ \\
Γ_u &= \sigma(W_u[c^{⟨t-1⟩}, x^{⟨t⟩}] + b_u)
\\ \\
c^{⟨t⟩} &= Γ_u \times \tilde c^{⟨t⟩} + (1 - Γ_u) \times c^{⟨t-1⟩}
\\ \\
a^{⟨t⟩} &= c^{⟨t⟩}
\end{aligned}
$$
$\tilde c^{⟨t⟩}$ 就是由上一层输出值 $c^{⟨t-1⟩}$ 计算出的候选值。

当使用 sigmoid 作为激活函数 $\sigma$ 来得到 $Γ_u$时，$Γ_u$ 的值在 0 到 1 的范围内，且大多数时间非常接近于 0 或 1。

当 $Γ_u = 1$时，$c^{⟨t⟩}$被更新为 $\tilde c^{⟨t⟩}$，更新。

当 $Γ_u = 0$时，$c^{⟨t⟩}$ 保持为 $c^{⟨t-1⟩}$，保留。

因为 $Γ_u$ 可以很接近 0，因此 $c^{⟨t⟩}$几乎就等于 $c^{⟨t-1⟩}$。在经过很长的序列后，$c$ 的值依然被维持，从而实现“记忆”的功能。

以上实际上是简化过的 GRU 单元，但是蕴涵了 GRU 最重要的思想。完整的 GRU 单元添加了一个新的**相关门（Relevance Gate）** $Γ_r$，表示 $\tilde c^{⟨t⟩}$和 $c^{⟨t⟩}$的相关性。因此，表达式改为如下所示：
$$
\begin{aligned}
\\ \\ 
\tilde c^{⟨t⟩} &= tanh(W_c[Γ_r \times c^{⟨t-1⟩}, x^{⟨t⟩}] + b_c) 
\\ \\ 
Γ_u &= \sigma(W_u[c^{⟨t-1⟩}, x^{⟨t⟩}] + b_u)
\\ \\ 
Γ_r &= \sigma(W_r[c^{⟨t-1⟩}, x^{⟨t⟩}] + b_r)
\\ \\ 
c^{⟨t⟩} &= Γ_u \times \tilde c^{⟨t⟩} + (1 - Γ_u) \times c^{⟨t-1⟩}
\\ \\ 
a^{⟨t⟩} &= c^{⟨t⟩}
\\ \\ 
\end{aligned}
$$
**GRU 的思路在于传播或更新上一层信息。**

论文：

1. [Cho et al., 2014. On the properties of neural machine translation: Encoder-decoder approaches](https://arxiv.org/pdf/1409.1259.pdf)
2. [Chung et al., 2014. Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](https://arxiv.org/pdf/1412.3555.pdf)

