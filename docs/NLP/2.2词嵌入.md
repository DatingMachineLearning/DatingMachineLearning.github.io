# 词嵌入

## 词汇表征（Word Representation）

one-hot 向量将每个单词表示为完全独立的个体，不同词向量都是**正交**的，因此单词间的相似度无法体现。换用特征化表示方法能够解决这一问题。我们可以通过用语义特征作为维度来表示一个词，因此语义相近的词，其词向量也相近。

将高维的词嵌入“嵌入”到一个二维空间里，就可以进行可视化。常用的一种可视化算法是 t-SNE 算法。在通过复杂而非线性的方法映射到二维空间后，每个词会根据语义和相关程度聚在一起。

相关论文：[van der Maaten and Hinton., 2008. Visualizing Data using t-SNE](https://www.seas.harvard.edu/courses/cs281/papers/tsne.pdf)

**词嵌入（Word Embedding）**是 NLP 中语言模型与表征学习技术的统称，概念上而言，它是指把一个维数为所有词的数量的高维空间（one-hot 形式表示的词）“嵌入”到一个维数低得多的连续向量空间中，每个单词或词组被映射为实数域上的向量。对大量词汇进行词嵌入后获得的词向量，可用于完成命名实体识别（Named Entity Recognition）等任务。

之所以叫嵌入的原因是，你可以想象一个300维的空间，我画不出来300维的空间，这里用个3维的代替。现在取每一个单词比如 orange，它对应一个3维的特征向量，所以这个词就被嵌在这个300维空间里的一个点上了，apple 这个词就被嵌在这个 300 维空间的另一个点上了。为了可视化，t-SNE 算法把这个空间映射到低维空间，你可以画出一个2维图像然后观察，这就是这个术语嵌入的来源。

## 词嵌入与迁移学习

用词嵌入做迁移学习可以降低学习成本，提高效率。其步骤如下：

1. 从大量的文本集中学习词嵌入，或者下载网上开源的、预训练好的词嵌入模型；
2. 将这些词嵌入模型迁移到新的、只有少量标注训练集的任务中；
3. 可以选择是否微调 (fine tune) 词嵌入。当标记数据集不是很大时可以省下这一步。

词嵌入可用于类比推理。例如，给定对应关系“男性（Man）”对“女性（Woman）”，想要类比出“国王（King）”对应的词汇。则可以有 $e_{man} - e_{woman} \approx e_{king} - e_? $ ，之后的目标就是找到词向量 $w$，来找到使相似度 $sim(e_w, e_{king} - e_{man} + e_{woman})$ 最大。

一个最常用的相似度计算函数是**余弦相似度（cosine similarity）**。公式为：

$$
sim(u, v)=\cos(u, v) = \frac{u^T v}{|| u ||_2 || v ||_2}
$$
相关论文：[Mikolov et. al., 2013, Linguistic regularities in continuous space word representations](

## 嵌入矩阵

![_20210825155227](img\_20210825155227.png)

不同的词嵌入方法能够用不同的方式学习到一个**嵌入矩阵（Embedding Matrix）** $E$。将字典中位置为 $i$ 的词的 one-hot 向量表示为 $o\_i$，词嵌入后生成的词向量用 $e\_i$表示，则有：
$$
E \cdot o_i = e_i
$$
但在实际情况下一般不这么做。因为 one-hot 向量维度很高，且几乎所有元素都是 0，这样做的效率太低。因此，实践中直接用专门的函数查找矩阵 $E$ 的特定列。例如在 Keras 中就有一个嵌入层，然后我们用这个嵌入层更有效地从嵌入矩阵中提取出你需要的列，而不是对矩阵进行很慢很复杂的乘法运算。

## 学习词嵌入

神经概率语言模型（Neural Probabilistic Language Model）构建了一个能够通过上下文来预测未知词的神经网络，在训练这个语言模型的同时学习词嵌入。

![image-20210825160405407](img\image-20210825160405407.png)

训练过程中，将语料库中的某些词作为目标词，以目标词的部分上下文作为输入，Softmax 输出的预测结果为目标词。嵌入矩阵 $E$ 和 $w$、$b$ 为需要通过训练得到的参数。这样，在得到嵌入矩阵后，就可以得到词嵌入后生成的词向量。

相关论文：[Bengio et. al., 2003, A neural probabilistic language model](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)

### Word2Vec

**Word2Vec** 是一种简单高效的词嵌入学习算法，包括 2 种模型：

* **Skip-gram (SG)**：根据词预测目标上下文
* **Continuous Bag of Words (CBOW)**：根据上下文预测目标词

每种语言模型又包含**负采样（Negative Sampling）**和**分级的 Softmax（Hierarchical Softmax）**两种训练方法。

训练神经网络时候的隐藏层参数即是学习到的词嵌入。

相关论文：[Mikolov et. al., 2013. Efficient estimation of word representations in vector space.](https://arxiv.org/pdf/1301.3781.pdf)

#### Skip-gram

![The-illustration-of-the-Skip-Gram-architecture-of-the-Word2Vec-algorithm-For-a_W640](img\The-illustration-of-the-Skip-Gram-architecture-of-the-Word2Vec-algorithm-For-a_W640.jpg)

*This figure was uploaded by [Hakime Öztürk](https://www.researchgate.net/profile/Hakime-Oeztuerk)*

![v2-ca81e19caa378cee6d4ba6d867f4fc7c_1440w](D:\git_repo\DatingMachineLearning.github.io\docs\NLP\img\v2-ca81e19caa378cee6d4ba6d867f4fc7c_1440w.png)

从上图可以看到，从左到右是 One-hot 向量，乘以 center word 的矩阵 $W$ 于是找到词向量，乘以另一个 context word 的矩阵 $W'$ 得到对每个词语的“相似度”，对相似度取 Softmax 得到概率，与答案对比计算损失。

设某个词为 $c$，该词的一定词距内选取一些配对的目标上下文 $t$，则该网路仅有的一个 Softmax 单元输出条件概率：

$$
p(t|c) = \frac{exp(\theta_t^T e_c)}{\sum^m_{j=1}exp(\theta_j^T e_c)}
$$
$\theta_t$ 是一个与输出 $t$ 有关的参数，其中省略了用以纠正偏差的参数。损失函数仍选用交叉熵：

$$
L(\hat y, y) = -\sum^m_{i=1}y_ilog\hat y_i
$$
在此 Softmax 分类中，每次计算条件概率时，需要对词典中所有词做求和操作，因此计算量很大。解决方案之一是使用一个**分级的 Softmax 分类器（Hierarchical Softmax Classifier）**，形如二叉树。在实践中，一般采用**霍夫曼树（Huffman Tree）**而非平衡二叉树，常用词在顶部。

如果在语料库中随机均匀采样得到选定的词 $c$，则 'the', 'of', 'a', 'and' 等出现频繁的词将影响到训练结果。因此，采用了一些策略来平衡选择。

#### CBOW

![CBOW](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Sequence_Models/CBOW.png)

CBOW 模型的工作方式与 Skip-gram 相反，通过采样上下文中的词来预测中间的词。

吴恩达老师没有深入去讲 CBOW。想要更深入了解的话，推荐资料： 

* [[NLP] 秒懂词向量Word2vec的本质](https://zhuanlan.zhihu.com/p/26306795)（中文，简明原理）
* [word2vec原理推导与代码分析-码农场](http://www.hankcs.com/nlp/word2vec.html)（中文，深入推导）

#### 负采样

![Defining-a-learning-problem](img\Defining-a-learning-problem.png)

为了解决 Softmax 计算较慢的问题，Word2Vec 的作者后续提出了负采样（Negative Sampling）模型。对于监督学习问题中的分类任务，在训练时同时需要正例和负例。在分级的 Softmax 中，负例放在二叉树的根节点上；而对于负采样，负例是随机采样得到的。

如上图所示，当输入的词为一对**上下文-目标词**时，标签设置为 1（这里的上下文也是一个词）。

另外任意取 k 对用相同的上下文词，与在字典中随机选取的词一起作为负样本，标签设置为 0。如果我们从字典中随机选到的词，正好出现在了词距内，比如说在上下文词**orange**正负10个词之内，也没关系。

**对于小数据集，k 取 5~20 较为合适；而当有大量数据时，k 可以取 2~5。**

改用多个 Sigmoid 输出上下文-目标词 $(c, t)$ 为正样本的概率，我们将使用记号 $c$ 表示上下文词，记号 $t$ 表示可能的目标词，我再用 $y$ 表示0 和 1，表示是否是一对上下文-目标词。我们要做的就是定义一个逻辑回归模型，给定输入的$c$，$t$对的条件下，$y=1$的概率，即：

$$
P(y=1 | c, t) = \sigma(\theta_t^Te_c)
$$
每一个可能的目标词有一个参数向量 $\theta_{t}$ ，和另一个参数向量 $e_{c}$，即每一个可能上下文词的的嵌入向量。

我们将用这个公式估计 $y=1$ 的概率。如果你有$K$个样本，你可以把这个看作 $\frac{1}{K}$ 的正负样本比例，即每一个正样本你都有$K$个对应的负样本来训练一个类似逻辑回归的模型。

之前训练中每次要更新 n 维的多分类 Softmax 单元（n 为词典中词的数量）。现在每次只需要更新 k+1 维的二分类 Sigmoid 单元，计算量大大降低。

关于计算选择某个词作为负样本的概率，作者推荐采用以下公式（而非经验频率或均匀分布）：

$$
p(w_i) = \frac{f(w_i)^{\frac{3}{4}}}{\sum^m_{j=0}f(w_j)^{\frac{3}{4}}}
$$


其中，$f(w_i)$ 代表语料库中单词 $w_i$ 出现的频率。上述公式更加平滑，能够增加低频词的选取可能。

相关论文：[Mikolov et. al., 2013. Distributed representation of words and phrases and their compositionality](https://arxiv.org/pdf/1310.4546.pdf)

### Glove

**GloVe（Global Vectors）**是另一种流行的词嵌入算法。Glove 模型基于语料库统计了词的**共现矩阵**$X$，$X$中的元素 $X_{ij}$ 表示单词 $i$ 和单词 $j$ “为上下文-目标词”的次数。之后，用梯度下降法最小化以下损失函数：
$$
J = \sum^N_{i=1}\sum^N_{j=1}f(X_{ij})(\theta^t_ie_j + b_i + b_j - log(X_{ij}))^2
$$


其中，$\theta_i$、$e_j$是单词 $i$ 和单词 $j$ 的词向量；$b_i$、$b_j$；$f()$ 是一个用来避免 $X_{ij}=0$时$log(X_{ij})$为负无穷大、并在其他情况下调整权重的函数。$X_{ij}=0$时，$f(X_{ij}) = 0$。

“为上下文-目标词”可以代表两个词出现在同一个窗口。在这种情况下，$\theta_i$ 和 $e_j$ 是完全对称的。因此，在训练时可以一致地初始化二者，使用梯度下降法处理完以后取平均值作为二者共同的值。

相关论文：[Pennington st. al., 2014. Glove: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf)

最后，使用各种词嵌入算法学到的词向量实际上大多都超出了人类的理解范围，难以从某个值中看出与语义的相关程度。
