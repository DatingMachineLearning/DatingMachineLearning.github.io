# 少样本学习的命名实体识别

a hierarchy of 8 coarse-grained and 66 fine-grained entity types. FEW-NERD consists of 188,238 sentences from Wikipedia, 4,601,160 words are included and each is annotated as context or a part of a two-level entity type. To the best of our knowledge, this is the first few-shot NER dataset and the largest human-crafted NER dataset. 

随着各个领域知识、命名实体的涌现，特别是需要专业知识才能理解的命名实体，很难进行大规模的手工注释。在这种情况下，研究能以很少的实例学习不可见实体类型的NER系统，即少样本 NER，在这方面起着至关重要的作用。然而现在并没有这样的数据集。

首先，大多数用于少镜头学习的数据集只有4- 18个粗粒度实体类型，这使得很难构建足够多的N-way**元任务**和学习**相关特征**。在现实中，我们观察到大多数看不见的实体都是细粒度的。

其次，由于缺乏基准数据集，不同作品的设置不一致 (Huang et al.， 2020;Yang和 Katiyar, 2020年)，导致不明确的比较。综上所述，这些方法对少样本的NER都有很好的贡献，但迫切需要一个特定的数据集来提供统一的基准数据集进行严格的比较

为了缓解上述挑战，我们提出了一个大规模的人工注释的少数场景的NER数据集，FEW-NERD，它包含从维基百科文章中提取的188.2万个句子和491.7万个实体，由训练良好的注释员手工注释。

对于基准设置，我们在FEW-NERD的基础上设计了三个任务，包括一个标准监督任务(FEW-NERD (SUP))和两个少样本学习任务(FEW- nerd - intra)和FEW-NRTD (INTER))。

FEW-NERD 是一个序列标注数据集，并没有考虑到嵌套、非连续、类型易混淆NER问题。

## 少样本 NER

在少样本的场景下，样本被按照批次（episode）组织成N-way-K-shot形式的数据。每个批次的数据又被组织成两个集合，support set 和 query set，其中support set用于学习，query set用于预测。其含义是，在每一批（episode）的support set中含有N种类型的实体，每种类型有K个实体，query set含有与support set同类型的实体。模型通过对support set的学习，来预测query set的标签。



> [浅谈嵌套命名实体识别（Nested NER） - 知乎](https://zhuanlan.zhihu.com/p/126347862)
>
> [nlp中的实体关系抽取方法总结 - 知乎](https://zhuanlan.zhihu.com/p/77868938)
>
> [一人之力，刷爆三路榜单！信息抽取竞赛夺冠经验分享](https://mp.weixin.qq.com/s/jqMWxrHwfMTpAjeJow_0JQ)

![图片](.\img\testw215)

- **嵌套NER**：例如在span「呼吸中枢受累」中，存在两个实体嵌套：「症状：呼吸中枢受累」和「部位：呼吸中枢」。我们在Q3中具体介绍解决方案。

- **非连续NER**：例如在span「尿道、膀胱、肾绞痛」中存在三个非连续实体「尿道痛」、「膀胱痛」、「肾绞痛」。这里给出3种解决方案：

- - 继续当作序列标注任务：拓展BIO标签；
  - 转化为一个属性/关系抽取问题：由于病历文本趋向模板化，所以用规则提取更加便捷。
  - 模仿句法解析器的做法，设置shift-reduce parser，具体可参见ACL20的这篇paper[4]。

- **类型易混淆NER**：例如对于部位实体「左肺上叶」，其归属于「病理」还是「影像」模块呢？对于「纵隔」部位，是属于「肿瘤」还是「淋巴结」部位呢？这里给出2种解决方案：

- - **事件论元抽取**：对于医疗领域，不同于通常的论元抽取，因为电子病历一般不存在**信息块重叠**（事件类型交叉重叠）问题，因此可以先进行**事件段落抽取**，再将「左肺上叶」部位实体归属到当前的事件段落中。
  - **两阶段NER**：在同一事件类型中，第一阶段可以确定**实体span边界**（例如找到部位实体「纵隔」的边界），第二阶段再结合上下文信息进行**实体typing**（例如对「纵隔」进行性质判断），这样做指标通常会提高哦～



# 嵌套实体问题

> NER是一个比较常见的NLP任务，通常采用LSTM+CRF处理一些简单NER任务。NER还存在嵌套实体问题（实体重叠问题），实体嵌套是指在一句文本中出现的实体，存在某个较短实体完全包含在另外一个较长实体内部的情况，如「《叶圣陶散文选集》」中会出现两个实体「叶圣陶」和「叶圣陶散文选集」分别代表「作者」和「作品」两个实体。而传统做法由于每一个token只能属于一种Tag，无法解决这类问题。下面归纳了几种常见并易于理解的解决办法：

## 序列标注：SoftMax 和 CRF

命名实体识别本来属于基于字（token-level）的多分类问题，通常采用CNNs/RNNs/BERT+CRF处理这类问题，与SoftMax相比，CRF进了标签约束。

- 针对CRF解码慢的问题，LAN[[1\]](https://zhuanlan.zhihu.com/p/77868938#ref_1)提出了一种逐层改进的基于标签注意力机制的网络，在保证效果的前提下比 CRF 解码速度更快。文中也发现BiLSTM-CRF在复杂类别情况下相比BiLSTM-softmax并没有显著优势。
- 由于分词边界错误会导致实体抽取错误，基于LatticeLSTM[[2\]](https://zhuanlan.zhihu.com/p/77868938#ref_2)+CRF的方法可引入词汇信息并避免分词错误（词汇边界通常为实体边界，根据大量语料构建词典，若当前字符与之前字符构成词汇，则从这些词汇中提取信息，联合更新记忆状态）。

但由于这种序列标注采取BILOU标注框架，每一个token只能属于一种，不能解决重叠实体问题，如图所示。

![img](https://pic3.zhimg.com/80/v2-135d19b9894df6ce265af8b37e3931fa_1440w.jpeg)

- **改进方法1**：采取 token-level 的多标签分类，将SoftMax替换为Sigmoid，如图所示。当然这种方式可能会导致：

1. label之间依赖关系的缺失，可采取后处理规则进行约束。
2. 学习难度较大

![img](https://pic2.zhimg.com/80/v2-67a484ce19e5f896393e862125ee2af1_1440w.jpg)

- **改进方法2**：依然采用CRF，但设置多个标签层，对于每一个token给出其所有的label，然后将所有标签层合并。基于这种方式，文献[[4\]](https://zhuanlan.zhihu.com/p/77868938#ref_4)也采取先验图的方式去解决重叠实体问题。有以下问题：

1. 增加label数量，导致label不平衡问题。
2. 指数级增加了标签
3. 对于多层嵌套，稀疏问题较为棘手

## Span抽取：指针网络

> 指针网络（PointerNet）最早应用于MRC中，而MRC中通常根据1个question从passage中抽取1个答案片段，转化为2个n元SoftMax分类预测头指针和尾指针。对于NER可能会存在多个实体Span，因此需要转化为n个2元Sigmoid分类预测头指针和尾指针。

将指针网络应用于NER中，可以采取以下两种方式：

第一种：MRC-QA+单层指针网络。构建query问题指代所要抽取的实体类型，同时也引入了先验语义知识。对不同实体类型构建query，并采取指针标注，此外也构建了矩阵来判断span是否构成一个实体mention。
如图所示，由于构建query问题已经指代了实体类型，所以使用单层指针网络即可；除了使用指针网络预测实体开始位置、结束位置外，还基于开始和结束位置对构成的所有实体Span预测实体概率。此外，这种方法也适合于给定事件类型下的事件主体抽取，可以将事件类型当作query，也可以将单层指针网络替换为CRF。

全匹配 0.7

部分匹配 高一点

