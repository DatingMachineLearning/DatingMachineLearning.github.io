# 文本预处理

## 输入

> [8.2. 文本预处理 — 动手学深度学习 2.0.0-alpha1 documentation](https://zh-v2.d2l.ai/chapter_recurrent-neural-networks/text-preprocessing.html)
>
> 文本是序列数据最常见例子。 例如，一篇文章可以简单地看作是一个单词序列，甚至是一个字符序列。 为了方便将来在实验中使用序列数据，我们将在本节中专门解释文本的常见预处理步骤。通常，这些步骤包括：
>
> 1. 将文本作为字符串加载到内存中。
> 2. 将字符串拆分为标记（如，单词和字符）。
> 3. 建立一个词汇表，将拆分的标记映射到数字索引。
> 4. 将文本转换为数字索引序列，以便模型可以轻松地对其进行操作。

所以文本应该先转换为计算机适合处理的方式，我们从 H.G.Well 的[时光机器](http://www.gutenberg.org/ebooks/35)中加载文本，命名为 `timemachine.txt`。

下面这段代码我们按文本中的标点符号断句（分割为一段段文本），并且统一所有单词为小写。

```python
import re
time_machine_path = "dataset/timemachine.txt"

def read_time_machine():
    with open(time_machine_path, 'r', encoding="utf-8") as f:
        lines = f.readlines()
        return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]

tm_content = read_time_machine()
print(f'# text lines: {len(tm_content)}')
print(tm_content[0])
print(tm_content[100])
```

## 标记化

简单来说，标记化 (tokenize) 就是将长字符串转化为字符串数组的过程，方便我们后续操作。

> *Tokens are the building blocks of Natural Language.* [What is Tokenization | Tokenization In NLP](https://www.analyticsvidhya.com/blog/2020/05/what-is-tokenization-nlp/)

标记化是一种将文本分成一个名为 token 的较小单位的方式。 在这里，token 可以是单词，字符或词根词缀。 因此，可以将标记分类为3种类型 —— 单词 (word)、字符 (char)、词根词缀（n-gram 字符）标记。

例如 "Never give up."

对于英文来说，形成令牌的最常见方式是基于空格。 假设空格作为分隔符，句子的标记导致3个令牌 `Never-give-up` 。 由于每个 token 都是一个单词，它成为单词标记化的示例。

同样的，比如 "smarter"：

1. 字符 (char) 标记化： `s-m-a-r-t-e-r`
2. 词根词缀标记化：`smart-er`

> 以下 tokenize 函数将列表作为输入，列表中的每个元素是一个文本序列（如，一条文本行）。每个文本序列被拆分成一个标记列表。*标记*（token）是文本的基本单位。最后返回一个标记列表，其中每个标记都是一个字符串（string）。

也就是转换为了 word 数组，或者是转换为 char 数组。分别代表了 单词 (word)、字符 (char) 标记化。如下：

```python
def tokenize(lines, token='word'):
    """将文本行拆分为单词或字符标记。"""
    if token == 'word':
        return [line.split() for line in lines]
    elif token == 'char':
        return [list(line) for line in lines]
    else:
        print('错误：未知令牌类型：' + token)

tokens = tokenize(tm_content, token='word')
for i in range(10):
    print(tokens[i])
```

输出：

```bash
['the', 'project', 'gutenberg', 'ebook', 'of', 'the', 'time', 'machine', 'by', 'h', 'g', 'wells']
[]
['this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'united', 'states', 'and']
['most', 'other', 'parts', 'of', 'the', 'world', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions']
['whatsoever', 'you', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 're', 'use', 'it', 'under', 'the', 'terms']
['of', 'the', 'project', 'gutenberg', 'license', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at']
['www', 'gutenberg', 'org', 'if', 'you', 'are', 'not', 'located', 'in', 'the', 'united', 'states', 'you']
['will', 'have', 'to', 'check', 'the', 'laws', 'of', 'the', 'country', 'where', 'you', 'are', 'located', 'before']
['using', 'this', 'ebook']
[]

Process finished with exit code 0
```

在中文中，因为汉字能组成各种各样的词语，中文标记化就是分词：

- 中文分词：指的是将一段文本拆分为一系列单词的过程，这些单词顺序拼接后等于原文本。
- 中文分词算法大致分为**基于词典规则**与**基于机器学习**这两大派。

### 基于字典的方法

> [Introduction-NLP/2.词典分词.md at master · NLP-LOVE/Introduction-NLP](https://github.com/NLP-LOVE/Introduction-NLP/blob/master/chapter/2.%E8%AF%8D%E5%85%B8%E5%88%86%E8%AF%8D.md#21-%E4%BB%80%E4%B9%88%E6%98%AF%E8%AF%8D)
>
> [8.2. 文本预处理 — 动手学深度学习 2.0.0-alpha1 documentation](https://zh-v2.d2l.ai/chapter_recurrent-neural-networks/text-preprocessing.html#id4)
>
> 标记的字符串类型不方便模型使用，因为模型需要的输入是数字。
>
> 现在，让我们构建一个**字典**，通常也叫做**词表（vocabulary）**，用来将字符串标记映射到从 0 开始的数字索引中。为此，我们首先统计训练集中所有文档中唯一的标记，称之为 **语料（corpus）**，然后根据每个唯一标记的出现频率为其分配一个数字索引。很少出现的标记通常被移除，这可以降低复杂性。语料库中不存在或已删除的任何标记都将映射到一个特定的未知标记 **<unk>** 。我们可以选择增加一个列表，用于保存保留的标记，例如**<pad>**表示填充；**<bos>**表示序列的开始；**<eos>**表示序列的结束。

```python
class Vocab:
    """文本词表"""

    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):
        if tokens is None:
            tokens = []
        if reserved_tokens is None:
            reserved_tokens = []
        # 按出现频率排序
        counter = count_corpus(tokens)
        self.token_freqs = sorted(counter.items(), key=lambda x: x[1],
                                  reverse=True)
        # 未知标记的索引为0
        self.unk, uniq_tokens = 0, ['<unk>'] + reserved_tokens
        uniq_tokens += [
            token for token, freq in self.token_freqs
            if freq >= min_freq and token not in uniq_tokens]
        self.idx_to_token, self.token_to_idx = [], dict()
        for token in uniq_tokens:
            self.idx_to_token.append(token)
            self.token_to_idx[token] = len(self.idx_to_token) - 1

    def __len__(self):
        return len(self.idx_to_token)

    #   查询文本序列的 token 数组
    def __getitem__(self, tokens):
        if not isinstance(tokens, (list, tuple)):
            return self.token_to_idx.get(tokens, self.unk)
        return [self.__getitem__(token) for token in tokens]

    #   将对应序号 token 输出，或者将一段切片序号的 token 输出
    def to_tokens(self, indices):
        if not isinstance(indices, (list, tuple)):
            return self.idx_to_token[indices]
        return [self.idx_to_token[index] for index in indices]
```

输出频率前十的 token 及其序号： 

```python
    vocab = Vocab(tokens)
    print([(vocab.idx_to_token[i], i) for i in range(10)])
```

结果：

```bash
[('<unk>', 0), ('the', 1), ('and', 2), ('of', 3), ('i', 4), ('a', 5), ('to', 6), ('in', 7), ('was', 8), ('that', 9)]
```

对于字典方法来说，还有很多策略，比如**完全切分**、**正向最长匹配**、**逆向最长匹配**、**双向最长匹配**。

![2020-2-4_14-15-53](img\2020-2-4_14-15-53.png)

还有内存占用低、速度快的**字典树**方法。[Introduction-NLP/2.词典分词.md at master · NLP-LOVE/Introduction-NLP](https://github.com/NLP-LOVE/Introduction-NLP/blob/master/chapter/2.%E8%AF%8D%E5%85%B8%E5%88%86%E8%AF%8D.md#21-%E4%BB%80%E4%B9%88%E6%98%AF%E8%AF%8D)

[中文分词的原理、方法与工具 - 知乎](https://zhuanlan.zhihu.com/p/146792308)

