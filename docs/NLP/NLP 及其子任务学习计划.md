èµ„æºï¼š

[å´æ©è¾¾ã€Šæ·±åº¦å­¦ä¹ ã€‹ä½œä¸šçº¿ä¸Šç‰ˆ - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/95510114)



[xmu-xiaoma666/External-Attention-pytorch: ğŸ€ Pytorch implementation of various Attention Mechanisms, MLP, Re-parameter, Convolution, which is helpful to further understand papers.â­â­â­](https://github.com/xmu-xiaoma666/External-Attention-pytorch)

[MuQiuJun-AI/bert4pytorch: è¶…è½»é‡çº§bertçš„pytorchç‰ˆæœ¬ï¼Œå¤§é‡ä¸­æ–‡æ³¨é‡Šï¼Œå®¹æ˜“ä¿®æ”¹ç»“æ„ï¼ŒæŒç»­æ›´æ–°](https://github.com/MuQiuJun-AI/bert4pytorch?continueFlag=e9813f68c7afd52a2d0bc48ec4ad1ab1)

[æé«˜ç§‘ç ”è®ºæ–‡å†™ä½œæ•ˆç‡çš„å°å·¥å…· - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/34838403)



## ä¹¦

[é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ (è±†ç“£)](https://book.douban.com/subject/35458428/)

[åŸºäºæ·±åº¦å­¦ä¹ çš„è‡ªç„¶è¯­è¨€å¤„ç† (è±†ç“£)](https://book.douban.com/subject/30236842/)

[NLPç®€ä»‹ - Science is interesting.](https://looperxx.github.io/NLP%E7%9A%84%E5%B7%A8%E4%BA%BA%E8%82%A9%E8%86%80/#46-bert)



## Papers

[thunlp/PLMpapers: Must-read Papers on pre-trained language models.](https://github.com/thunlp/PLMpapers)

[datawhalechina/learn-nlp-with-transformers: we want to create a repo to illustrate usage of transformers in chinese](https://github.com/datawhalechina/learn-nlp-with-transformers)

[datawhalechina/daily-interview: Datawhaleæˆå‘˜æ•´ç†çš„é¢ç»ï¼Œå†…å®¹åŒ…æ‹¬æœºå™¨å­¦ä¹ ï¼ŒCVï¼ŒNLPï¼Œæ¨èï¼Œå¼€å‘ç­‰ï¼Œæ¬¢è¿å¤§å®¶star](https://github.com/datawhalechina/daily-interview)






æˆ‘ç»å†äº†NLPçš„çº¯è‡ªå­¦è¿‡ç¨‹ï¼Œæˆ‘ä¸ªäººè§‰å¾—ç®—æ˜¯æ¯”è¾ƒå¿«æ·çš„æ–¹æ³•ã€‚

å…¶ä¸­ï¼Œä¹ŸæŠŠä¸­æ–‡èµ„æ–™å’Œæ–¯å¦ç¦çš„CS224Nè¯¾ç¨‹ç›¸ç»“åˆï¼Œå¯¹NLPåšäº†ä¸€ä¸ªå…¥é—¨å­¦ä¹ å’Œç®€å•æ·±å…¥ï¼Œæ¨èçœ‹çœ‹æˆ‘å†™çš„è¿™ç¯‡æ€»ç»“æ–‡ç« 



## Re-invent Bertç³»åˆ—

| å¹´ä»½ | è®ºæ–‡æ ‡é¢˜                                                     | å…³é”®è¯                                                       |
| ---- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 2014 | Memory Network                                               | Memory                                                       |
| 2015 | Neural Machine Translation by Jointly Learning to Align and  Translate Attention | Attention                                                    |
| 2015 | Effective Approaches to Attention-based Neural Machine  Translation | use LSTM                                                     |
| 2015 | End-To-End     Memory Networks                               | multi step attention(k-hop as RNN, global memory)            |
| 2016 | Googleâ€™s     Neural Machine Translation System               | low-precision, wordpieces                                    |
| 2016 | A Convolutional Encoder Model for Neural     Machine Translation | CNN encoder                                                  |
| 2017 | Language Modeling with Gated Convolutional Networks          | CNN & GLU(compared with self-attention)                      |
| 2017 | Convolutional     Sequence to Sequence Learning              | Multiple stack, parallel                                     |
| 2017 | Attention is all you need                                    | http://jalammar.github.io/illustrated-transformer/     http://nlp.seas.harvard.edu/2018/04/03/attention.html |
| 2018 | Deep contextualized word representations                     | ELMo                                                         |
| 2018 | Improving Language Understanding     by Generative Pre-Training | GPT                                                          |
| 2018 | BERT: Pre-training of Deep Bidirectional Transformers for Language  Understanding | Bert                                                         |
| 2018 | Language Models are Unsupervised Multitask Learners          | GPT-2                                                        |
| 2019 | RoBERTa: A Robustly Optimized BERT Pretraining Approach      | RoBERTa                                                      |
|      |                                                              |                                                              |
|      |                                                              |                                                              |
|      |                                                              |                                                              |
|      |                                                              |                                                              |







## å‘¨è®¡åˆ’

7æœˆ22æ—¥ - 7æœˆ23æ—¥ä¸¤å¤© å…¥é—¨ï¼š

- äº†è§£ nlp çš„ä¼ ç»Ÿæ–¹æ³•å’ŒåŸºæœ¬æ¦‚å¿µã€‚

- æŒæ¡ tokenizationï¼ˆåˆ†è¯ï¼‰æ–¹æ³•ã€‚
- äº†è§£ jieba, spacy åˆ†è¯æ–¹æ³•ï¼Œè·‘ä¾‹å­ã€‚
- äº†è§£å‘½åå®ä½“è¯†åˆ«ã€æƒ…æ„Ÿåˆ†æã€å…³ç³»æå–çš„åŸºæœ¬è¿‡ç¨‹ä¸æ¦‚å¿µã€‚

ç¬¬äºŒå‘¨ è¯å‘é‡ï¼š

- åºåˆ—æ ‡æ³¨æ¨¡å‹ï¼Œ CRF
- çŸ¥é“å¸¸ç”¨çš„è¯åµŒå…¥æ–¹æ³•ã€‚

- äº†è§£ Word2Vec æ¨¡å‹ã€‚
- åˆ©ç”¨CRF ã€Word2Vec  æ¨¡å‹å®ç°å¯¹åº”åŠŸèƒ½ï¼Œè·‘ä¾‹å­ã€‚

ç¬¬ä¸‰å‘¨ è¯å‘é‡ï¼š

- äº†è§£ count based global matrix factorization ã€‚
- äº†è§£ glove æ¨¡å‹ã€‚
- åˆ©ç”¨ä¸Šè¿°æ¨¡å‹å®ç°å¯¹åº”åŠŸèƒ½ï¼Œè·‘ä¾‹å­ã€‚

ç¬¬å››å‘¨ å­è¯æ¨¡å‹ï¼š

- äº†è§£ n-gram æ€æƒ³å’Œ FastText æ¨¡å‹ã€‚
- åˆ©ç”¨ä¸Šè¿°æ¨¡å‹å®ç°å¯¹åº”åŠŸèƒ½ï¼Œè·‘ä¾‹å­ã€‚

ç¬¬äº”å‘¨ ä¸Šä¸‹æ–‡è¯åµŒå…¥æ¨¡å‹ï¼š

- äº†è§£ contextual word representation
- äº†è§£ ELMOï¼ŒGPT ä¸ BERT æ¨¡å‹
- åˆ©ç”¨ä¸Šè¿°æ¨¡å‹å®ç°å¯¹åº”åŠŸèƒ½ï¼Œè·‘ä¾‹å­ã€‚





## å…¶ä»–

**ä¸­æ–‡è¯å‘é‡çš„æ¢ç´¢**

- ç»ƒä¹ ä»»åŠ¡
- ç‰¹å¾è¯è½¬åŒ–ä¸º One-hot çŸ©é˜µ
- ç‰¹å¾è¯è½¬åŒ–ä¸º tdidf çŸ©é˜µ
- åˆ©ç”¨ word2vec è¿›è¡Œ è¯å‘é‡è®­ç»ƒ
- word2vec ç®€å•åº”ç”¨
- åˆ©ç”¨ one-hot ã€TF-idfã€word2vec æ„å»ºå¥å‘é‡ï¼Œç„¶å é‡‡ç”¨ æœ´ç´ è´å¶æ–¯ã€æ¡ä»¶éšæœºåœºåšåˆ†ç±»



ä¼ ç»Ÿç®—æ³•

- ç¼–è¾‘è·ç¦»ï¼šæŒ‡ä¸¤ä¸ªå­—ç¬¦ä¸²ä¹‹é—´ï¼Œç”±ä¸€ä¸ªè½¬æˆå¦ä¸€ä¸ªæ‰€éœ€çš„æœ€å°‘ç¼–è¾‘æ“ä½œæ¬¡æ•°
- é›†åˆåº¦é‡ç‰¹å¾ï¼šåŸºäºBOW(bag of words)ï¼Œåˆ©ç”¨é›†åˆç›¸ä¼¼æ€§åº¦é‡æ–¹æ³•ï¼Œå¦‚Jaccard
- ç»Ÿè®¡ç‰¹å¾ï¼šå¦‚å¥å­é•¿åº¦ã€è¯ä¸ªæ•°ã€æ ‡ç‚¹æ•°é‡ã€æ ‡ç‚¹ç±»å‹ã€è¯æ€§é¡ºåºç­‰
- è¯å‘é‡ï¼šå°†ä¸¤ä¸ªæ–‡æœ¬è¡¨ç¤ºæˆåŒä¸€å‘é‡ç©ºé—´ä¸­çš„å‘é‡ï¼Œè®¡ç®—æ¬§å¼è·ç¦»ã€ä½™å¼¦ç›¸ä¼¼åº¦ç­‰
- åˆ©ç”¨TF/IDF/LDAè¡¨ç¤ºè¿›è¡Œåº¦é‡ï¼šå¦‚BM25æ–‡æœ¬ç›¸ä¼¼åº¦

å…¶ä¸­åŸºäºç‰¹å¾çš„æ–¹æ³•ï¼Œæ•ˆæœå¾ˆä¾èµ–ç‰¹å¾çš„è®¾è®¡ã€‚åŸºäºä¼ ç»Ÿæ£€ç´¢æ¨¡å‹çš„æ–¹æ³•å­˜åœ¨ä¸€ä¸ªå›ºæœ‰ç¼ºé™·ï¼Œå°±æ˜¯æ£€ç´¢æ¨¡å‹æ™ºèƒ½å¤„ç†Queryä¸Documentæœ‰é‡åˆè¯çš„æƒ…å†µï¼Œæ— æ³•å¤„ç†è¯è¯­çš„è¯­ä¹‰ç›¸å…³æ€§ã€‚



æ·±åº¦ç®—æ³•

- Siamese Network
- DSSMï¼ˆDeep Semantic Structured Modelï¼‰
- CDSSMï¼ˆConvolutional Deep Semantic Structured Modelï¼‰
- ARC-I: Convolutional Neural Network Architectures for Matching Natural Language Sentences
- RNNï¼ˆRecurrent Neural Networksï¼‰
- RNNå˜ç§ï¼šLSTMã€Match-LSTMã€Seq-to-Seqã€Attention
- DeepMatch
- ARC-II
- MatchPyramid
- Match-SRNN