# 模型评估与选择（数据集构建）

## 机器学习过程

回顾高一的运动学实验，探索运动学公式:

$$
x = \frac{1}{2}at^2
$$
把带有滑轮的长木板平放在实验桌上，把滑轮伸出桌面，把打点计时器固定在长木板上没有滑轮的一端，并把打点计时器连接在电源上。此时 $a = g = 10  m/s^2$



```python
import numpy as np
import matplotlib.pyplot as plt

x = np.linspace(0, 20, 20)
y = 0.5 * 10 * (x**2)
trainning_set = y + np.random.randn(y.shape[-1]) * 2.5
plt.plot(x, trainning_set, 'gx')


plt.show()
```


![png](https://gitee.com/xrandx/blog-figurebed/raw/master/img/20210311132510.png)
    



训练集 $X = \{(x_i, y_i)\}$ 这里等于 $X = \{(0, 0), (1, 20.6), (3, 45.2), \cdots \}$

测试集 

假设空间：一元一次函数，一元 n 次函数，牛顿的假设-一元二次函数




```python
def poly_format(coeff):
    fmt = ["%.2f" % (coeff[-1])]
    cnt = -1
    for i in reversed(coeff):
        cnt += 1
        if cnt == 0:
            continue
        fmt.append("%.2fx^%d + " % (i, cnt))
    fmt = reversed(fmt)
    return "".join(fmt)


```


```python
coeff = np.polyfit(x, trainning_set, 2)
print(poly_format(coeff))

poly2 = np.polyval(coeff, x)
plt.plot(x, trainning_set, 'gx')
plt.plot(x, poly2, 'b')
plt.show()
```

    5.01x^2 + -0.22x^1 + 0.64




![png](https://gitee.com/xrandx/blog-figurebed/raw/master/img/20210311132518.png)
    



```python
coeff = np.polyfit(x, trainning_set, 1)
print(poly_format(coeff))
poly1 = np.polyval(coeff, x)
plt.plot(x, trainning_set, 'gx')
plt.plot(x, poly1, 'b')
plt.plot(x, test_set, 'rx')
plt.show()
```

    100.03x^1 + -315.95




![png](https://gitee.com/xrandx/blog-figurebed/raw/master/img/20210311132521.png)
    



```python
coeff = np.polyfit(x, trainning_set, 3)
print(poly_format(coeff))
poly1 = np.polyval(coeff, x)
plt.plot(x, trainning_set, 'gx')
plt.plot(x, poly1, 'b')
plt.plot(x, test_set, 'rx')

plt.show()
```

    -0.00x^3 + 5.08x^2 + -0.73x^1 + 1.37




![png](https://gitee.com/xrandx/blog-figurebed/raw/master/img/20210311132524.png)
    



## 飞浆案例：牛顿第二定律

牛顿第二定律是艾萨克·牛顿在1687年于《自然哲学的数学原理》一书中提出的，其常见表述：物体加速度的大小跟作用力成正比，跟物体的质量成反比，与物体质量的倒数成正比。牛顿第二运动定律和第一、第三定律共同组成了牛顿运动定律，阐述了经典力学中基本的运动规律。

在中学课本中，牛顿第二定律有两种实验设计方法：倾斜滑动法和水平拉线法：

![img](https://gitee.com/xrandx/blog-figurebed/raw/master/img/20210510082936.jpeg)

所示的不同作用力下的木块加速度：

<img src="https://gitee.com/xrandx/blog-figurebed/raw/master/img/20210510082923.png" alt="img" style="zoom:50%;" />

观察实验数据不难猜测，物体的加速度 a 和作用力 *F* 之间的关系应该是线性关系。因此我们提出假设 ，其中，*a*代表加速度，*F*代表作用力，*w*是待确定的参数。

举例类比，机器如一个机械的学生一样，只能通过尝试答对（最小化损失）大量的习题（已知样本）来学习知识（模型参数*W*），并期望用学习到的知识（模型参数*W*）所代表的模型 *H*(*W*,*X*)，回答不知道答案的考试题（未知样本）。最小化损失是模型的优化目标，实现损失最小化的方法称为优化算法，也称为寻解算法（找到使得损失函数最小的参数解）。参数*W*和输入*X*组成公式的基本结构称为假设。在牛顿第二定律的案例中，基于对数据的观测，我们提出了线性假设，即作用力和加速度是线性关系，用线性方程表示。

**模型假设、评价函数（损失/优化目标）和优化算法是构成模型的三个部分**。

![img](https://gitee.com/xrandx/blog-figurebed/raw/master/img/20210510082443.png)

- **模型假设**：世界上的可能关系千千万，漫无目标的试探*Y*~*X*之间的关系显然是十分低效的。因此假设空间先圈定了一个模型能够表达的关系可能，如蓝色圆圈所示。机器还会进一步在假设圈定的圆圈内寻找最优的*Y*~*X*关系，即确定参数*W*。
- **评价函数**：寻找最优之前，我们需要先定义什么是最优，即评价一个*Y*~*X*关系的好坏的指标。通常衡量该关系是否能很好的拟合现有观测样本，将拟合的误差最小作为优化目标。
- **优化算法**：设置了评价指标后，就可以在假设圈定的范围内，将使得评价指标最优（损失函数最小/最拟合已有观测样本）的*Y*~*X*关系找出来，这个寻找的方法即为优化算法。最笨的优化算法即按照参数的可能，穷举每一个可能取值来计算损失函数，保留使得损失函数最小的参数作为最终结果。

从上述过程可以得出，机器学习的过程与牛顿第二定律的学习过程基本一致，都分为假设、评价和优化三个阶段：

1. **假设**：通过观察加速度$a$ 和作用力 F 的观测数据，假设 a 和 F 是线性关系，即 $a=w \cdot F $。
2. **评价**：对已知观测数据上的拟合效果好，即 $w \cdot F$ 计算的结果，要和观测的 $a$ 尽量接近。
3. **优化**：在参数 $w$ 的所有可能取值中，发现 $w=1/m$ 可使得评价最好（最拟合观测样本）。

机器执行学习的框架体现了其**学习的本质是“参数估计”**（Learning is parameter estimation）。在此基础上，许多看起来完全不一样的问题都可以使用同样的框架进行学习，如科学定律、图像识别、机器翻译和自动问答等，它们的学习目标都是拟合一个“大公式”。

## 模型归纳偏好

特化与泛化

没有免费的午餐：https://www.leiphone.com/news/201907/jswawIEtorcAYvrB.html

奥卡姆剃刀原则：如无必要，勿增实体

## 误差

我们将学习器对样本的实际预测结果与样本的真实值之间的差异成为：误差（error）。

 - 在训练集上的误差称为训练误差（training error）或经验误差（empirical error）。
 - 在测试集上的误差称为测试误差（test error）。
 - 学习器在所有新样本上的误差称为泛化误差（generalization error）。

显然，我们希望得到的是在新样本上表现得很好的学习器，即泛化误差小的学习器。因此，我们应该让学习器尽可能地从训练集中学出普适性的“一般特征”，这样在遇到新样本时才能做出正确的判别。然而，当学习器把训练集学得“太好”的时候，即把一些训练样本的自身特点当做了普遍特征；同时也有学习能力不足的情况，即训练集的基本特征都没有学习出来。我们定义：

 - 学习能力过强，以至于把训练样本所包含的不太一般的特性都学到了，称为：过拟合（overfitting）。
 - 学习能太差，训练样本的一般性质尚未学好，称为：欠拟合（underfitting）。

## 训练集与测试集的构建方法

我们希望用一个“测试集”的“测试误差”来作为“泛化误差”（因为不可能知道）的近似，因此我们需要对初始数据集进行有效划分，划分出互斥的“训练集”和“测试集”。下面介绍几种常用的划分方法：


### 留出法

将数据集D划分为两个互斥的集合，一个作为训练集 $S$，一个作为测试集$T$ ，满足 $D=S∪T$ 且 $S∩T=∅$

常见的划分为：大约2/3-4/5的样本用作训练，剩下的用作测试。需要注意的是：训练/测试集的划分要尽可能保持数据分布的一致性，以避免由于分布的差异引入额外的偏差，常见的做法是采取分层抽样。同时，由于划分的随机性，单次的留出法结果往往不够稳定，一般要采用若干次随机划分，重复实验取平均值的做法。

### 交叉验证法

 将数据集 $D$ 划分为两个互斥的集合，一个作为训练集 $S$，一个作为测试集 $T$，满足 $D=S∪T且S∩T=∅$ ，常见的划分为：大约$2/3-4/5$的样本用作训练，剩下的用作测试。需要注意的是：训练/测试集的划分要尽可能保持数据分布的一致性，以避免由于分布的差异引入额外的偏差，常见的做法是采取分层抽样。同时，由于划分的随机性，单次的留出法结果往往不够稳定，一般要采用若干次随机划分，重复实验取平均值的做法。

 ### 自助法
我们希望评估的是用整个D训练出的模型。但在留出法和交叉验证法中，由于保留了一部分样本用于测试，因此实际评估的模型所使用的训练集比D小，这必然会引入一些因训练样本规模不同而导致的估计偏差。留一法受训练样本规模变化的影响较小，但计算复杂度又太高了。“自助法”正是解决了这样的问题。

自助法的基本思想是：给定包含m个样本的数据集D，每次随机从D 中挑选一个样本，将其拷贝放入D'，然后再将该样本放回初始数据集D 中，使得该样本在下次采样时仍有可能被采到。重复执行m 次，就可以得到了包含m个样本的数据集D'。 

 ### 调参
大多数学习算法都有些参数(parameter) 需要设定，参数配置不同，学得模型的性能往往有显著差别，这就是通常所说的"参数调节"或简称"调参" (parameter tuning)。

学习算法的很多参数是在实数范围内取值，因此，对每种参数取值都训练出模型来是不可行的。常用的做法是：对每个参数选定一个范围和步长λ，这样使得学习的过程变得可行。

## ML1答案

1. BCD
2. 特征选择错了

答案可见 ML3 视频开头